{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**MICRO-452 Final Report**](#toc0_)\n",
    "### <a id='toc1_1_1_'></a>[*Group 45: Anton Balykov, Teo Halevi, Cyprien Lacassagne, Seonmin Park*](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Table of Contents](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Project Objective](#toc3_)    \n",
    "- [I. Computer Vision](#toc4_)    \n",
    "  - [I.1 Introduction](#toc4_1_)    \n",
    "  - [I.2 Camera Set Up](#toc4_2_)    \n",
    "  - [I.3 Global Obstables and Thymio Detection](#toc4_3_)    \n",
    "- [II. Global Navigation](#toc5_)    \n",
    "  - [II.1 Introduction](#toc5_1_)    \n",
    "  - [II.2 Path Planning](#toc5_2_)    \n",
    "  - [II.3 Algorithm](#toc5_3_)    \n",
    "- [III. Local Navigation](#toc6_)    \n",
    "  - [III.1 Introduction](#toc6_1_)    \n",
    "  - [III.1 Local Avoidance](#toc6_2_)    \n",
    "- [IV. Filtering](#toc7_)    \n",
    "  - [IV.1 Introduction](#toc7_1_)    \n",
    "  - [IV.2 Methodology](#toc7_2_)    \n",
    "  - [IV.3 Dynamics of Thymio](#toc7_3_)    \n",
    "  - [IV.4 Design and Implementation of Filter](#toc7_4_)    \n",
    "  - [IV.5 Simulation](#toc7_5_)    \n",
    "- [Conclusion](#toc8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Project Objective](#toc0_)\n",
    "\n",
    "This project leverages the Thymio robot to explore and integrate key concepts introduced in the Introduction to Mobile Robotics course. We hereby focus on implementing and demonstrating core components: computer vision, global navigation, motion control, local navigation, and filtering. The mission of the robot is to navigate itself from the designated start position to the specified goal position, successfully avoiding fixed obstacles as well as dynamically placed one. Achieving this requires precise motion control, robust obstable detection, and efficient filtering technique. Such practical application of the five above-mentioned elements would reflect our team's comprehensive understanding of mobile robotics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[I. Computer Vision](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[I.1 Introduction](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc4_2_'></a>[I.2 Camera Set Up](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc4_3_'></a>[I.3 Global Obstables and Thymio Detection](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[II. Global Navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[II.1 Introduction](#toc0_)\n",
    "\n",
    "One main component of this project is navigation. Here, we discuss about our implementation of the global navigation, whose main objective is to find a collision-free path from a source to a goal, on a map filled with polygons acting as obtacles. The environment of motion is imply a 2D flat surface such as the ground or a table. The global, and thus **persistant** obstacles are red polygons made of paper, and placed freely by hand to test the robustness of the path planning algorithm. The boundaries of our map are determined by the field of view - and thus the height - of the camera from the ground, as regions that lie outside the visible field are not processed. To find the optimal path from the robot to the goal, the strategy relies on a metric map and visibility graph, as well as Dijkstra's graph search algorithm.\n",
    "\n",
    "Global navigation is only involved in the beginning of the program, outside of the main loop. The only case where global path is updated is when the robot is \"kidnapped\", i.e. moved by hand to another position on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[II.2 Initial Strategy](#toc0_)\n",
    "\n",
    "Initially, we decided to go with a discretized fixed-size cell-map and use Dijktra's search algorithm. Although the implementation was not difficult, we thought that it was not as elegant as the more intuitive visibility graph approach. Nonetheless, here is the result of this preliminary work:\n",
    "\n",
    "![Cell_decomposition](report-images/cell_decomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[II.3 Polygon Expansion](#toc0_)\n",
    "\n",
    "We decided to clearly separate features detection (`ComputerVision`) and path planning (`Navigation`).\n",
    "The class `Navigation` is constructed by receiving the position of the robot (source), of the goal, and of the vertices of each obstacle. All of this data is extracted from the camera image by the class `ComputerVision`, as explained in Part I. Each obstacle conists of a list of points in the form `[x, y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvisgraph as vg\n",
    "\n",
    "THYMIO_RADIUS = 70 # mm\n",
    "\n",
    "class Navigation:\n",
    "    def __init__(self, obstacles, robot, goal):\n",
    "        \"\"\"\n",
    "        :param obstacles: A list of all the obstacles on the map. An obstacle is a list of \n",
    "                            vertices, ordered CCW.\n",
    "        :param robot: The position of the robot.\n",
    "        :param goal: The position of the goal.\n",
    "        \"\"\"\n",
    "        self.obstacles = obstacles\n",
    "        self.obstacles_count = len(obstacles)\n",
    "        self.extended_obstacles = []\n",
    "        self.source = robot\n",
    "        self.goal = goal\n",
    "        self.path = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into graph creation, the polygons need to be expanded in order to account for the size of the robot. Otherwise, each obstacle's vertex would be registered as a node, and thus a potential waypoint in the final path, and would be mapped to the center of the robot. This is not ideal since the robot would bump into the obstacles. The function `augment_obstacles` loops through the polygons and computes two new points for each vertex, expanded once in the direction of each of the edges that make up the vertex, and once in the direction of their normals, by a constant factor `THYMIO_RADIUS`, which is the radius of the robot in mm. That way, we ensure the robot always keeps a safe minimal distance with the obstacles, even when some portion of the resulting path is tengential to a polygon edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def augment_obtacles(self):\n",
    "    \"\"\"\n",
    "    Augments the countour of the obstacles w.r.t. the radius of the robot.\n",
    "    \"\"\"\n",
    "    for polygon in self.obstacles:\n",
    "        extended_polygon = []\n",
    "\n",
    "        count = len(polygon)\n",
    "        for i in range(count):\n",
    "            vertex = polygon[i]\n",
    "            prev = polygon[i-1]\n",
    "            next = (polygon[i+1] if i < count-1 else polygon[0])\n",
    "\n",
    "            edge1 = np.subtract(vertex, prev)\n",
    "            edge2 = np.subtract(next,vertex)\n",
    "\n",
    "            dir1 = np.array(edge1 / np.linalg.norm(edge1))\n",
    "            dir2 = np.array(edge2 / np.linalg.norm(edge2))\n",
    "\n",
    "            perp1 = -np.array([dir1[1], -dir1[0]])\n",
    "            perp2 = -np.array([dir2[1], -dir2[0]])\n",
    "\n",
    "            v1 = vertex + THYMIO_RADIUS * dir1 + THYMIO_RADIUS * perp1\n",
    "            v2 = vertex - THYMIO_RADIUS * dir2 + THYMIO_RADIUS * perp2\n",
    "\n",
    "            # Preserve the winding of the original polygon\n",
    "            angle = np.arccos(np.clip(np.dot(-dir1, dir2), -1.0, 1.0))\n",
    "            if angle <= np.pi / 2.0:\n",
    "                extended_polygon.append(v1)\n",
    "                extended_polygon.append(v2)\n",
    "            else:\n",
    "                extended_polygon.append(v2)\n",
    "                extended_polygon.append(v1)\n",
    "\n",
    "        self.extended_obstacles.append(extended_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result of our implemantation of polygon expansion. In this example, we set the vertices and thymio's radius to arbitrary values.\n",
    "\n",
    "![Polygon_expansion](report-images/Polygon_expansion.png)\n",
    "\n",
    "Expanding the vertices by two points instead of only one, allows for a smoother bypassing and reduces the total path length, while increasing the accuracy of the Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[II.4 Visibility Graph & Graph Search](#toc0_)\n",
    "\n",
    "Then, a visibility graph is built using the open-source library [pyvisgraph](https://github.com/TaipanRex/pyvisgraph). This library uses [Lee's algorithm](https://dav.ee/papers/Visibility_Graph_Algorithm.pdf), O(n²log(n)), to compute the visibility of each node.<br>\n",
    "This operation will fail if the winding of the obstacles is incoherent. That is why we take care of expanding the corners of the obstacles in the right order.\n",
    "\n",
    "Finally, the shortest path is computed using Dijkstra's algorithm, and returned to the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(self):\n",
    "    \"\"\"\n",
    "    Computes the shortest path from source (robot) to goal, while avoiding the obstacles.\n",
    "\n",
    "    :returns:\n",
    "        A list of the coordinates of each node from robot position to goal.\n",
    "    \"\"\"\n",
    "    self.augment_obtacles(self)\n",
    "\n",
    "    graph = vg.VisGraph()\n",
    "    polygons = []\n",
    "    for obstacle in self.extended_obstacles:\n",
    "        polygon = []\n",
    "        for point in obstacle:\n",
    "            polygon.append(vg.Point(point[0], point[1]))\n",
    "\n",
    "        polygons.append(polygon)\n",
    "            \n",
    "    graph.build(polygons)\n",
    "    path = graph.shortest_path(vg.Point(self.source[0], self.source[1]), vg.Point(self.goal[0], self.goal[1]))\n",
    "\n",
    "    for point in path:\n",
    "        self.path.append(point)\n",
    "        print(point)\n",
    "\n",
    "    return self.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='toc5_5_'></a>[II.5 Results](#toc0_)\n",
    "\n",
    "Below are the results of two different configurations that shows how the path changes when the first shortest path (left) is blocked (right). A pass through between two obstacles is choked when at least one vertex of a polygon lies into the expanded version of the other one, as you can see on the right image, where we lowered the middle triangle by 50 units.\n",
    "\n",
    "![path_planning_1](report-images/path_planning_1.png) ![path_planning_2](report-images/path_planning_2.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[III. Local Navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[III.1 Introduction](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc6_2_'></a>[III.1 Local Avoidance](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[IV. Filtering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[IV.1 Introduction](#toc0_)\n",
    "\n",
    "\n",
    "There is a need for a method that compensates the defect that there is no direct mapping between Thymio's sensors and the values which are used for control. The absolute position of the robot can be obtained directly from the camera and global path planning techniques, but the robot may have low precision and update rate, causing drifts when the velocity or acceleration is integrated. We can address this challenge by implementing Kalman filter as it provides a sound framework to use all available measurements in an optimal way. It relies on a model for the system structure and the effect of disturbances both on the state and on the measurements.\n",
    "\n",
    "The Kalman filter we implemented estimates the position and speed of the robot when it moves along the planned path. Specifically, we implemented extended Kalman filter to take into account the system involves nonlinear dynamics that would need to be linearized around the current estimate, focusing on local optimality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[IV.2 Methodology](#toc0_)\n",
    "\n",
    "The Thymio robot moves at constant speed across a pattern made of black and white stripes of 50 mm, identical to one of the activities found in Exercise 8 of the course. The actual mean velocity of the two wheels and the mean reflected light read by the two ground sensors are sent in an event at a constant rate. The python code receives these values, updates the estimated velocity and position of the robot, and displays them continuously in a plot.\n",
    "\n",
    "\n",
    "To obtain velocity parameters, we began with estimating Thymio's speed by let it cross seven blocks that are 50 mm each with a sample rate of 0.1 seconds and marked peaks of the ground sensor meaasuremnet indicating that it has crossed evey block. With known and obtained information, we could obtain the conversion factor of 0.4 to conver Thymio's speed to mm/s.\n",
    "\n",
    "![Find Peaks](find_peaks.png \"Fine Peaks\")\n",
    "\n",
    "Now, to obtain speed variance, we observed the data where the robot was moving by plotting the robot's speed and computed average velocity and thus the speed variance by diving the average speed with the robot's speed in mm/s.\n",
    "\n",
    "![Measured Vel](measured_vel.png \"Measured Vel\")\n",
    "\n",
    "With the assumption that half of the variance is cause by the measurement and the other half is caused by the perturbation to the states, we obtain \n",
    "\n",
    "$q_v = r_v = 16.08621$ $mm^2/s^2$ $\\space$ where $\\space$ $\\sigma^2_{vel} = 32.17242$ $mm^2/s^2$\n",
    "\n",
    "Variances on position state and measurement as well as those for the angle for orientation ($\\sigma^2_{pos}$ and $\\sigma^2_{\\theta}$) were an arbitrary value that were tuned manually for the specific Thymio robot that is going to be used for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_3_'></a>[IV.3 Dynamics of Thymio](#toc0_)\n",
    "\n",
    "To begin with the implementation, the state space model is\n",
    "\n",
    "$\\hat{x}_{k-1|k-1} =  \\begin{bmatrix}\n",
    "                        x_{pos_{k-1}} \\\\\n",
    "                        y_{pos_{k-1}} \\\\\n",
    "                        \\theta_{k-1} \\\\\n",
    "                        v_{left_{k-1}} \\\\\n",
    "                        v_{right_{k-1}} \\\\\n",
    "                        \\end{bmatrix}$\n",
    "\n",
    "Under control input vector,\n",
    "\n",
    "$ u =[v_{trans}, v_{rot}] $, $\\space$ where $\\space$ $v_{tran} = \\frac{v_{left} + v_{right}}{2} $ $\\space$ and $\\space$ $ v_{rot} = \\frac{v_{left} - v_{right}}{l_{base}} $\n",
    "\n",
    "Then, we can see that the robot's new position and orientation are predicted in the following way\n",
    "\n",
    "$x_{pos_{k}} = x_{pos_{k-1}} + v_{trans_{k}} \\cdot \\cos(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$y_{pos_{k}} = y_{pos_{k-1}} + v_{trans_{k}} \\cdot \\sin(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$\\theta_{k} = \\theta_{k-1} + v_{rot_{k}} \\cdot dk$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_4_'></a>[IV.4 Design and Implementation of Filter](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced that the extended Kalman filter takes into account nonlinear system dynamics in the previous sections. In order to make prediction of the next state, We use a nonlinear motion model $f(x, u)$. That is, we can formulate their relation as:\n",
    "\n",
    "$\\hat{x}_{k|k-1} = f(\\hat{x}_{k-1|k-1}, u_k)$\n",
    "\n",
    "Our $f(x, u)$ is then introduced as\n",
    "\n",
    "$f(\\hat{x}_{k-1|k-1}, u_k) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & -0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & -0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & 0 & -0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & -0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -\\frac{dk}{2 \\cdot l_{\\text{base}}} & \\frac{dk}{2 \\cdot l_{\\text{base}}} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this prediction step of the filter, we make a use of the predicted covariance matrix $P_{k-1|k-1}$.\n",
    "\n",
    "$P_{k-1|k-1} = \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as each column corresponds to the state variables, from the first column to the last column, being $x_{pos_{k-1}}$, $y_{pos_{k_1}}$, $\\theta_{k-1} $, $v_{left_{k-1}}$, and $v_{right_{k-1}}$, respectively.\n",
    "\n",
    "The updated (current) covariance matrix is as follows:\n",
    "\n",
    "$P_{k|k-1} = F_{k}P_{k-1|k-1}F_{k}^{T} + Q_{k}$\n",
    "\n",
    "where $F_{k}$ is the Jacobian of $f_{k}$ matrix, and $Q_{k}$ is the noise covariance matrix of the state model. The $Q_{k}$ matrix specifically represents the degree or the extend of how much the actual motion of Thymio deviates from its assumed state space model which would be the path that it should follow for our case.\n",
    "\n",
    "We can write the two matrices as below:\n",
    "\n",
    "$F_{k|k-1} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & v_{trans} \\cdot \\sin(\\theta_{k-1}) \\cdot dk & -0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & -0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & v_{trans} \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -\\frac{dk}{2 \\cdot l_{base}} & \\frac{dk}{2 \\cdot l_{base}} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Q_{k|k-1} = \\alpha \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\alpha$ is a scaling factor, which we have initialized as $\\alpha =10$, such that \n",
    "\n",
    "$Q_{k-1|k-1} = 10 \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "We set $\\alpha$ large enough so that the filter tracks large changes in the wheel sensor measurements more closely as if it was smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class ***KalmanFilterExtended*** and the class is called in the **main.py** file where all the five components of the project are integrated.\n",
    "\n",
    "The function ***run_ekf()*** ensures that the extened filter is intiated and the snippet from **main.py** below ensures that the filter is running in a loop throughout the entire mission of the robot.\n",
    "\n",
    "Additionally, we have set parameters that defines *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. Specifically, we have set the threshold for orientation and position to be 35cm and 60°, and exceeding either (or both) of these values would be considered getting kidnaped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Codes to be displayed here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_5_'></a>[IV.5 Simulation](#toc0_)\n",
    "\n",
    "to be shown later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Conclusion](#toc0_)\n",
    "It has been an incredible journey to embark on this project within the constrained timeline of just three weeks, replete with challenges and invaluable learning opportunities. From the very beginning, we have had to face numerous obstacles, such as technical errors and conceptual hurdles, which have pushed us to refine our problem-solving skills and deepen our understanding of mobile robotics. Despite these challenges, our team has been able to overcome setbacks and achieve results we are proud of through persistence, discipline, and a collaborative spirit.\n",
    "\n",
    "Working through the complexities of the project was as rewarding as it was demanding. From understanding the theoretical basis to the implementation of practical solutions, each step provided an opportunity to reinforce and showcase our grasp of the principles of mobile robotics. The hands-on nature of the project tested not only our knowledge but also helped us grow as engineers, fostering resilience, adaptability, and creativity in tackling real-world problems.\n",
    "\n",
    "We would like to express our deep gratitude to everyone who guided and supported us through this journey. We would like to thank Professor Mondada, who inspired us with his experience and vision, and the teaching assistants, who were always ready to help, explain doubts, and give us valuable insights. Their encouragement and mentorship played a very important role in driving the success of our project.\n",
    "\n",
    "It is not only an academic milestone but also a source of personal and professional growth, and we will be carrying the lessons learned herein as we progress in our future endeavors related to robotics and beyond."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
