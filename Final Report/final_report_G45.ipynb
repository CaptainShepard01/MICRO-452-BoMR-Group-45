{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**MICRO-452 Final Report**](#toc0_)\n",
    "### <a id='toc1_1_1_'></a>[*Group 45: Anton Balykov, Teo Halevi, Cyprien Lacassagne, Seonmin Park*](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Table of Contents](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Project Objective](#toc0_)    \n",
    "- [I. Computer Vision](#toc1_)    \n",
    "  - [I.1 Introduction](#toc1_1_)    \n",
    "  - [I.2 Camera Set Up](#toc1_2_)    \n",
    "  - [I.3 Global Obstables and Thymio Detection](#toc1_3_)    \n",
    "- [II. Global Navigation](#toc2_)    \n",
    "  - [II.1 Introduction](#toc2_1_)    \n",
    "  - [II.2 Path Planning](#toc2_2_)    \n",
    "  - [II.3 Algorithm](#toc2_3_)    \n",
    "- [III. Local Navigation](#toc3_)    \n",
    "  - [III.1 Introduction](#toc3_1_)    \n",
    "  - [III.1 Local Avoidance](#toc3_2_)    \n",
    "- [IV. Filtering](#toc4_)    \n",
    "  - [IV.1 Introduction](#toc4_1_)    \n",
    "  - [IV.2 Methodology](#toc4_2_)    \n",
    "  - [IV.3 Dynamics of Thymio](#toc4_3_)    \n",
    "  - [IV.4 Design and Implementation of Filter](#toc4_4_)    \n",
    "  - [IV.5 Simulation](#toc4_5_)    \n",
    "- [Conclusion](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc0_'></a>[Project Objective](#toc0_)\n",
    "\n",
    "This project leverages the Thymio robot to explore and integrate key concepts introduced in the Introduction to Mobile Robotics course. We hereby focus on implementing and demonstrating core components: computer vision, global navigation, motion control, local navigation, and filtering. The mission of the robot is to navigate itself from the designated start position to the specified goal position, successfully avoiding fixed obstacles as well as dynamically placed one. Achieving this requires precise motion control, robust obstable detection, and efficient filtering technique. Such practical application of the five above-mentioned elements would reflect our team's comprehensive understanding of mobile robotics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[I. Computer Vision](#toc1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[I.1 Introduction](#toc1_1_)\n",
    "\n",
    "Computer vision is the first step in our project, as it is the basic input. To steer the robot in an environment, we need to know the robot's position in that environment, what obstacles are present, where to go, and whether data acquisition is proceeding correctly. This is the purpose of the `ComputerVision` class we have implemented. All this data will then be used to predict the optimal path from the starting position to the goal, avoiding obstacles, and to provide an initial estimate of the position to be given to the Kalmann filter.\n",
    "\n",
    "We distinguish several parts in this explanation: the general setup, including the camera, the creation of the map, and the creation of markers to detect the robot and the goal. We then talk about detecting obstacles in the map, which the robot must avoid. Finally, the detection of the robot and the goal is a crucial part, which will be done using ArUco markers. \n",
    "\n",
    "## <a id='toc1_2_'></a>[I.2 Setup : camera, map, and markers](#toc1_2_)\n",
    "\n",
    "The first thing to create is the environment in which our robot will evolve. This environment is made up of delimited obstacles in a free zone for the robot. After several computer vision tests, we decided that our obstacles would be represented by red shapes on a white background. This choice is explained in greater detail in the following section on obstacle detection. Once our map has been created, we need to find a way of finding the robot in the image. To do this, we chose to use the ArUco markers provided by the OpenCV library in Python. These markers are QR-code-like representations with unique identifiers that can be easily detected in space. By defining a marker for the robot, and a marker for the goal, we can extract their respective positions to define the robot's shortest path to the goal.\n",
    "\n",
    "ArUco markers are defined by two things: a dictionary and an identifier. In our case, we use the dictionary `DICT_6X6_250` and IDs 4 and 8 for the robot and goal respectively. A marker is generated as follows, where `marker_id` is the defined ID of the marker, and `marker_size` is the size of the marker in pixels:\n",
    "```python\n",
    "dictionary   = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "marker_image = cv2.aruco.generateImageMarker(dictionary, marker_id, marker_size)\n",
    "```\n",
    "\n",
    "To have enough markers to test the robustness of our code, we created and printed a grid of 9 markers, for IDs from 0 to 8.\n",
    "\n",
    "<img src=\"report-images/aruco_grid.png\" alt=\"aruco_grid\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "The webcam used for our project has a resolution of 1920 by 1080 pixels. The camera is positioned above and in the center of the card, to minimize image distortion. An example of the setup with map, obstacles, robot, robot and goal markers and camera is shown below.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"Original frame from the webcam\" width=\"750\" class=\"center\"/>\n",
    "\n",
    "When using a camera, two parameters need to be determined: the camera matrix $C$ and the distortion coefficients $D$. Matrix $C$ contains the camera's intrinsic parameters: focal lengths and optical center, along the $x$ and $y$ axes. These parameters are  denoted by: $f_x$, $f_y$ (focal lengths) and $c_x$, $c_y$ (optical center), which gives us: $$C = \\begin{pmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "The distorsion matrix $D$ is defined as $D = \\begin{pmatrix} k_1 & k_2 & p_1 & p_2 & k_3 \\end{pmatrix}$ where the parameters are defined in [Camera Calibration](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html).\n",
    "\n",
    "Thw two matrices $C$ and $D$ are found using calibration. This step is usually performed using a reference image such as a checkerboard. We used a checkerboard with 9 by 7 inner corners, and with $20$mm square length. By taking several photos from different angles and heights of the checkerboard, we can correct the distorted image. An image of the calibration process is shown below.\n",
    "\n",
    "<img src=\"report-images/checkerboard_calibration.png\" alt=\"checkerboard_calibration.\" width=\"750\" class=\"center\"/>\n",
    "\n",
    "For our purposes, we have decided to set the origin at the top left corner of the image. To achieve this, the optical centers $c_x$ and $c_y$ were set to $0$. In addition, distortion coefficients were considered negligible when comparing data with and without calibration. Once these matrices are known, the camera is initialized and can be used to detect obstacles and ArUco markers.\n",
    "\n",
    "## <a id='toc1_3_'></a>[I.3 Detection of global obstacles](#toc1_3)\n",
    "\n",
    "To be able to steer the robot along the shortest path, we need to know its environment. This means knowing the location of global obstacles on the map.\n",
    "After several tests, we decided to create our obstacles using flat red shapes. This decision is part of our global obstacle detection strategy.\n",
    "\n",
    "To detect obstacles in the image, the first step is to apply a mask to the image to extract only the red areas. To do this, we work in the HSV domain of the image. We have determined that the different shades of red can belong to two intervals defined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_red_1 = np.array([0, 100, 100])\n",
    "upper_red_1 = np.array([10, 255, 255])\n",
    "\n",
    "lower_red_2 = np.array([170, 100, 100])\n",
    "upper_red_2 = np.array([180, 255, 255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function `apply_color_mask(frame, mask_color='n')` creates the image's red masks for each of the two intervals using the `cv2.inRange` verification function. To bring the masks together, we compare the images pixel by pixel using the OR operator. This returns an image with values of $255$ (white) for pixels detected as red, and $0$ (black) everywhere else.  Finally, the mask undergoes the morphological operation `cv2.MORPH_CLOSE` to reunite disjointed red areas or complete black pixels present in white areas.\n",
    "\n",
    "The result of this red mask is shown below. We recall the original image. Note that only the red shapes are shown in the black and white image.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"original_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "<img src=\"report-images/red_masked_frame.jpg\" alt=\"red_masked_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "This implementation allows us to assume that all obstacles in the image are red. This will facilitate their detection and enable ideal robot \n",
    "control.\n",
    "\n",
    "Once the red shapes have been extracted, we are interested in extracting their edges. Indeed, this is the only important information about an obstacle, since the obstacle's edge must not be exceeded. To extract the edges, we use a Canny edge detector implemented in OpenCV:\n",
    "```python\n",
    "cv2.Canny(red_mask, 50, 150)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/red_edges_frame.jpg\" alt=\"red_edges_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "The Canny edge detector involves several stages: noise reduction using Gaussian blurring, gradient calculation, non-maximum suppression, and edge tracking by hysteresis. The second and third arguments in `cv2.Canny` represent the lower and upper thresholds for the hysteresis step. Pixels with gradient intensities above the upper threshold are classified as edges, while those below the lower threshold are ignored. Pixels with intensities between the thresholds are included as edges only if they are connected to a strong edge. The limit values found were obtained after several tests to extract edges on different shades of red, and appear to be optimal for our application.\n",
    "\n",
    "To predict the shortest path between the robot and the goal, we use a visibility graph. To use it, we need to know the corners of each obstacle. Our function `get_corners_and_shape_edges(edges)` use a polygonal approximation of the shapes found. This approximation allows each obstacle to be represented as a polygon, and therefore to have the corners of these polygons. To study each contour separately, we use `cv2.findContours`. \n",
    "```python\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "\n",
    "Each contour is then approximated as a polygon, with a certain tolerance. Since each shape is approximated by polygons, rounded shapes can also be defined as a list of corners. This is the case for the ellipse in the figures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate the contour to a polygon\n",
    "epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "approx  = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "# Extract corner point from the approximated polygon\n",
    "corners = [[int(point[0][0]), int(point[0][1])] for point in approx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the list of corners returned by our function will have the form:\n",
    "```python\n",
    "[[[a1,b1], [a2, b2], [a3, b3]],                      # x and y coordinate of the first obstacle\n",
    " [[x1, y1], [x2, y2], [x3, y3], [x4, y4], [x5, y5]], # x and y coordinate of the second obstacle\n",
    " [[o1, p1], [o2, p2], [o3, p3], [o4, p4]]]           # x and y coordinate of the third obstacle\n",
    "```\n",
    "\n",
    "The display of detected contours and corners gives the following image.\n",
    "\n",
    "<img src=\"report-images/red_corners_obstacles_frame.jpg\" alt=\"red_corners_obstacles_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "And by displaying the corners of the red obstacles in green on the original camera image, we obtain the following visualization:\n",
    "\n",
    "<img src=\"report-images/original_frame_with_corners_obstacles.jpg\" alt=\"original_frame_with_corners_obstacles\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "Note that the above functions give the positions of obstacle corners in pixels. These positions will be converted to millimetres to maintain consistency in the project's units. This conversion is performed using ArUco markers.\n",
    "\n",
    "## <a id='toc1_4_'></a>[I.4 Detection of the robot and the goal](#toc1_4_)\n",
    "\n",
    "Global obstacle detection is now implemented. The map in which the robot is moving is therefore known, and obstacles are clearly positioned. The missing step in directing the robot is to know its position, as well as the position of its goal. \n",
    "\n",
    "As mentioned in the introduction, we use ArUco markers to identify the robot and the goal. For the following explanations, we'll use the image below with markers 7 (top left), 3 (top right), and 5 (bottom). The photo has been deliberately taken at an angle, with marker 5 blurred to demonstrate the robustness of marker detection.\n",
    "\n",
    "<img src=\"report-images/aruco_3markers.jpg\" alt=\"aruco_3markers\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "Basic marker detection is performed by the `cv2.aruco.detectMarkers` function, which takes as arguments the frame to be analyzed, the dictionary used for markers (in our case `DICT_6X6_250`), and initialized parameters. This function returns a list of marker corners, as well as the identifiers of recognized markers.\n",
    "\n",
    "In the ComputerVision class, the marker detection and marker drawing operation is performed by the following lines, in the function `detect_and_estimate_pose()` and gives the image below:\n",
    "```python\n",
    "corners, marker_ids, _ = cv2.aruco.detectMarkers(frame_with_markers, self.aruco_dict, parameters=self.aruco_params)\n",
    "cv2.aruco.drawDetectedMarkers(frame_with_markers, corners, marker_ids)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/aruco_3markers_with_id.jpg\" alt=\"aruco_3markers_with_id\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "We can see that the markers are detected correctly: they are surrounded by a green border, their ID is shown in blue, and fourth corner of each marker is shown in red.\n",
    "\n",
    "The positioning of the red corner in the image above gives an indication of the next step in the detection process. The corners of a marker are defined in the following order:\n",
    "1. Bottom left\n",
    "2. Bottom right\n",
    "3. Top right\n",
    "4. Top left (red corner shown)\n",
    "\n",
    "where the reference for top, bottom, left, right is given by the conventional reading of the text above the marker.\n",
    "\n",
    "To extract the position of markers in mm, use the function `cv2.aruco.estimatePoseSingleMarkers` as follows: \n",
    "```python\n",
    "rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners,\n",
    "                                                      self.ARUCO_MARKER_SIDE_LENGTH,\n",
    "                                                      self.mtx,\n",
    "                                                      self.dst)\n",
    "```\n",
    "\n",
    "The `self.ARUCO_MARKER_SIDE_LENGTH` parameter defined in class initialization is the length of a marker side in mm. It is used to switch from pixel coordinates to mm coordinates. Note also that the `mtx` and `dst` matrices are used, corresponding respectively to the $C$ and $D$ matrices defined in the calibration explained above.\n",
    "\n",
    "All this is done by calling the function:\n",
    "```python\n",
    "frame_with_markers, marker_ids, rvecs, tvecs, aruco_diagonal_pixels = vision.detect_and_estimate_pose(frame_marker)\n",
    "```\n",
    "\n",
    "The result contains two variables `rvecs` (rotation vectors) and `tvecs` (translation vectors).\n",
    "The rotation vector (`rvecs`) is first converted into a rotation matrix, which provides a representation of 3D orientation. A $4\\times 4$ identity matrix is initialized to serve as a transformation matrix, with its top-left $3\\times 3$ portion allocated for the rotation information. The function `cv2.Rodrigues()` is then used to convert the rotation vector into a $3\\times 3$ rotation matrix.\n",
    "\n",
    "Next, the $3\\times 3$ rotation matrix is converted into a quaternion using the `scipy.spatial.transform.Rotation.from_matrix()` method. A quaternion is a four-dimensional representation $[x, y, z, w]$ of rotation. In the quaternion, $x$, $y$, and $z$ represent the vector part, while $w$ represents the scalar part. Finally, the quaternion is converted to Euler angles using the function `euler_from_quaternion`. This function calculates roll (rotation around the $x$-axis), pitch (rotation around the $y$-axis), and yaw (rotation around the $z$-axis). Among these, the yaw angle is extracted because it represents the rotation of the marker in the $x$-$y$ plane, which corresponds to the marker's orientation relative to the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_marker_pose(self, frame, marker_ids, rvecs, tvecs):\n",
    "    \"\"\"\n",
    "    Process pose information for each detected marker and draw on a copy of the original frame.\n",
    "\n",
    "    original_frame: The frame to process\n",
    "    marker_ids, rvecs, tvecs: Detected marker details (IDs, rotation vectors, translation vectors)\n",
    "    mtx, dst: Camera matrix and distortion coefficients\n",
    "\n",
    "    Returns:\n",
    "        frame_with_vectors: A new frame with the vectors drawn\n",
    "        markers_data: Data of the markers (x-position, y-position, yaw-angle for each marker)\n",
    "    \"\"\"\n",
    "\n",
    "    frame_with_vectors = frame.copy()\n",
    "    markers_data = []\n",
    "\n",
    "    for i, marker_id in enumerate(marker_ids):\n",
    "        transform_translation_x = 1000*(tvecs[i][0][0]) # 1000* to have in millimeters.\n",
    "        transform_translation_y = 1000*(tvecs[i][0][1])\n",
    "        transform_translation_z = 1000*tvecs[i][0][2]\n",
    "\n",
    "        rotation_matrix = np.eye(4)\n",
    "        rotation_matrix[0:3, 0:3] = cv2.Rodrigues(np.array(rvecs[i][0]))[0]\n",
    "        r = R.from_matrix(rotation_matrix[0:3, 0:3])\n",
    "        quat = r.as_quat()\n",
    "\n",
    "        roll_x, pitch_y, yaw_z = self.euler_from_quaternion(quat[0], quat[1], quat[2], quat[3])\n",
    "\n",
    "        roll_x = math.degrees(roll_x)\n",
    "        pitch_y = math.degrees(pitch_y)\n",
    "        yaw_z = -(math.degrees(yaw_z)-90)\n",
    "\n",
    "        markers_data.extend([[marker_id[0], transform_translation_x, transform_translation_y, yaw_z]])\n",
    "\n",
    "        cv2.drawFrameAxes(frame_with_vectors, self.mtx, self.dst, rvecs[i], tvecs[i], 0.05)\n",
    "\n",
    "    return frame_with_vectors, markers_data\n",
    "\n",
    "\n",
    "def euler_from_quaternion(x, y, z, w):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "    roll is rotation around x in radians (counterclockwise)\n",
    "    pitch is rotation around y in radians (counterclockwise)\n",
    "    yaw is rotation around z in radians (counterclockwise)\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    roll_x = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = np.clip(t2, -1.0, 1.0)\n",
    "    pitch_y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "    yaw_z = math.atan2(t3, t4)\n",
    "\n",
    "    return roll_x, pitch_y, yaw_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the function `process_marker_pose` defined above, we obtain the frame with the detected vectors, as well as the coordinates of each marker.\n",
    "\n",
    "```python\n",
    "frame_with_vectors, markers_data = vision.process_marker_pose(frame_with_markers, marker_ids, rvecs, tvecs)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/aruco_3markers_with_vectors.jpg\" alt=\"aruco_3markers_with_vectors\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "For this image, the coordinates are as follows, with ID, $x$-coordinate (mm), $y$-coordinate (mm) and yaw-angle (degrees) for each marker.\n",
    "\n",
    "```python\n",
    "[[5, 166.79759785976782, 139.66100169531774, 97.87032032706453],\n",
    " [7, 148.05928528540807, 66.24491386374977, 120.86243771133647],\n",
    " [3, 296.46820843077455, 67.33202952024861, -45.05418817033191]]\n",
    "```\n",
    "\n",
    "The yaw angle is defined as 0 when the green vector is vertical, and follows the clockwise direction. $x$ increases when the marker goes to the right of the image, and $y$ increases when the marker goes to the bottom of the image.\n",
    "\n",
    "Returning to the original image with the Thymio, we can now display all the information: obstacles, obstacle corners, robot and goal.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"original_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "<img src=\"report-images/frame_aruco_and_corners.jpg\" alt=\"frame_aruco_and_corners\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "\n",
    "**NOTE: About conversion from pixels to mm**\n",
    "\n",
    "ArUco marker coordinates are given directly in mm by the function `detect_and_estimate_pose()`.\n",
    "On the other hand, the coordinates of obstacle corners are given in pixels.\n",
    "\n",
    "To have mm everywhere and ensure consistency, we need to convert the coordinates of the obstacle corners into mm. This is done by finding the pixel-mm conversion factor. To do this, we need a length that is both known in mm and known in pixels. We use the information contained in the ArUco markers. In fact, the corners list used in the `detect_and_estimate_pose()` function contains the pixel coordinates of the corners of each marker. Taking the first marker detected, we can calculate the length of its diagonal in pixels. With the actual length of the diagonal in mm known, the conversion factor is found and used to convert the coordinates of the obstacle corners into mm:\n",
    "\n",
    "```python\n",
    "aruco_diagonal_pixels = math.sqrt((corners[0][0][2][1] - corners[0][0][0][1])**2 + (corners[0][0][2][0] - corners[0][0][0][0])**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[I.5 Detection of kidnapping](#toc1_5_)\n",
    "\n",
    "The last part for the `ComputerVision` class is to detect whether the camera has been covered, or whether the robot has been kidnapped and repositioned elsewhere. We rely on the detection of the marker assigned to the robot to create the corresponding function. If the marker assigned to the robot does not belong to the list of detected markers, then the function returns `True` (to say that the robot has been kidnapped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camera_covered(self, detected_markers):\n",
    "    \"\"\"\n",
    "    Returns True (kidnapped) if the ARUCO_ROBOT_ID is not detected, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    if detected_markers is None:\n",
    "        return True\n",
    "    else:\n",
    "        return self.ARUCO_ROBOT_ID not in detected_markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[II. Global Navigation](#toc2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[II.1 Introduction](#toc2_1_)\n",
    "\n",
    "One main component of this project is navigation. Here, we discuss about our implementation of the global navigation, whose main objective is to find a collision-free path from a source to a goal, on a map filled with polygons acting as obtacles. The environment of motion is imply a 2D flat surface such as the ground or a table. The global, and thus **persistant** obstacles are red polygons made of paper, and placed freely by hand to test the robustness of the path planning algorithm. The boundaries of our map are determined by the field of view - and thus the height - of the camera from the ground, as regions that lie outside the visible field are not processed. To find the optimal path from the robot to the goal, the strategy relies on a metric map and visibility graph, as well as Dijkstra's graph search algorithm.\n",
    "\n",
    "Global navigation is only involved in the beginning of the program, outside of the main loop. The only case where global path is updated is when the robot is \"kidnapped\", i.e. moved by hand to another position on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[II.2 Initial Strategy](#toc2_2_)\n",
    "\n",
    "Initially, we decided to go with a discretized fixed-size cell-map and use Dijktra's search algorithm. Although the implementation was not difficult, we thought that it was not as elegant as the more intuitive visibility graph approach. Nonetheless, here is the result of this preliminary work:\n",
    "\n",
    "![Cell_decomposition](report-images/cell_decomposition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[II.3 Polygon Expansion](#toc2_3_)\n",
    "\n",
    "We decided to clearly separate features detection (`ComputerVision`) and path planning (`Navigation`).\n",
    "The class `Navigation` is constructed by receiving the position of the robot (source), of the goal, and of the vertices of each obstacle. All of this data is extracted from the camera image by the class `ComputerVision`, as explained in Part I. Each obstacle conists of a list of points in the form `[x, y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvisgraph as vg\n",
    "\n",
    "THYMIO_RADIUS = 70 # mm\n",
    "\n",
    "class Navigation:\n",
    "    def __init__(self, obstacles, robot, goal):\n",
    "        \"\"\"\n",
    "        :param obstacles: A list of all the obstacles on the map. An obstacle is a list of \n",
    "                            vertices, ordered CCW.\n",
    "        :param robot: The position of the robot.\n",
    "        :param goal: The position of the goal.\n",
    "        \"\"\"\n",
    "        self.obstacles = obstacles\n",
    "        self.obstacles_count = len(obstacles)\n",
    "        self.extended_obstacles = []\n",
    "        self.source = robot\n",
    "        self.goal = goal\n",
    "        self.path = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into graph creation, the polygons need to be expanded in order to account for the size of the robot. Otherwise, each obstacle's vertex would be registered as a node, and thus a potential waypoint in the final path, and would be mapped to the center of the robot. This is not ideal since the robot would bump into the obstacles. The function `augment_obstacles` loops through the polygons and computes two new points for each vertex, expanded once in the direction of each of the edges that make up the vertex, and once in the direction of their normals, by a constant factor `THYMIO_RADIUS`, which is the radius of the robot in mm. That way, we ensure the robot always keeps a safe minimal distance with the obstacles, even when some portion of the resulting path is tengential to a polygon edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def augment_obtacles(self):\n",
    "    \"\"\"\n",
    "    Augments the countour of the obstacles w.r.t. the radius of the robot.\n",
    "    \"\"\"\n",
    "    for polygon in self.obstacles:\n",
    "        extended_polygon = []\n",
    "\n",
    "        count = len(polygon)\n",
    "        for i in range(count):\n",
    "            vertex = polygon[i]\n",
    "            prev = polygon[i-1]\n",
    "            next = (polygon[i+1] if i < count-1 else polygon[0])\n",
    "\n",
    "            edge1 = np.subtract(vertex, prev)\n",
    "            edge2 = np.subtract(next,vertex)\n",
    "\n",
    "            dir1 = np.array(edge1 / np.linalg.norm(edge1))\n",
    "            dir2 = np.array(edge2 / np.linalg.norm(edge2))\n",
    "\n",
    "            perp1 = -np.array([dir1[1], -dir1[0]])\n",
    "            perp2 = -np.array([dir2[1], -dir2[0]])\n",
    "\n",
    "            v1 = vertex + THYMIO_RADIUS * dir1 + THYMIO_RADIUS * perp1\n",
    "            v2 = vertex - THYMIO_RADIUS * dir2 + THYMIO_RADIUS * perp2\n",
    "\n",
    "            # Preserve the winding of the original polygon\n",
    "            angle = np.arccos(np.clip(np.dot(-dir1, dir2), -1.0, 1.0))\n",
    "            if angle <= np.pi / 2.0:\n",
    "                extended_polygon.append(v1)\n",
    "                extended_polygon.append(v2)\n",
    "            else:\n",
    "                extended_polygon.append(v2)\n",
    "                extended_polygon.append(v1)\n",
    "\n",
    "        self.extended_obstacles.append(extended_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result of our implemantation of polygon expansion. In this example, we set the vertices and thymio's radius to arbitrary values.\n",
    "\n",
    "![Polygon_expansion](report-images/Polygon_expansion.png)\n",
    "\n",
    "Expanding the vertices by two points instead of only one, allows for a smoother bypassing and reduces the total path length, while increasing the accuracy of the Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[II.4 Visibility Graph & Graph Search](#toc2_4)\n",
    "\n",
    "Then, a visibility graph is built using the open-source library [pyvisgraph](https://github.com/TaipanRex/pyvisgraph). This library uses [Lee's algorithm](https://dav.ee/papers/Visibility_Graph_Algorithm.pdf), O(n²log(n)), to compute the visibility of each node.<br>\n",
    "This operation will fail if the winding of the obstacles is incoherent. That is why we take care of expanding the corners of the obstacles in the right order.\n",
    "\n",
    "Finally, the shortest path is computed using Dijkstra's algorithm, and returned to the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(self):\n",
    "    \"\"\"\n",
    "    Computes the shortest path from source (robot) to goal, while avoiding the obstacles.\n",
    "\n",
    "    :returns:\n",
    "        A list of the coordinates of each node from robot position to goal.\n",
    "    \"\"\"\n",
    "    self.augment_obtacles(self)\n",
    "\n",
    "    graph = vg.VisGraph()\n",
    "    polygons = []\n",
    "    for obstacle in self.extended_obstacles:\n",
    "        polygon = []\n",
    "        for point in obstacle:\n",
    "            polygon.append(vg.Point(point[0], point[1]))\n",
    "\n",
    "        polygons.append(polygon)\n",
    "            \n",
    "    graph.build(polygons)\n",
    "    path = graph.shortest_path(vg.Point(self.source[0], self.source[1]), vg.Point(self.goal[0], self.goal[1]))\n",
    "\n",
    "    for point in path:\n",
    "        self.path.append(point)\n",
    "        print(point)\n",
    "\n",
    "    return self.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='toc2_5_'></a>[II.5 Results](#toc2_5)\n",
    "\n",
    "Below are the results of two different configurations that shows how the path changes when the first shortest path (left) is blocked (right). A pass through between two obstacles is choked when at least one vertex of a polygon lies into the expanded version of the other one, as you can see on the right image, where we lowered the middle triangle by 50 units.\n",
    "\n",
    "![path_planning_1](report-images/path_planning_1.png) ![path_planning_2](report-images/path_planning_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[III. Local Navigation](#toc3_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[III.1 Introduction](#toc3_1_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc3_2_'></a>[III.2 Local Avoidance](#toc3_2_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[IV. Filtering](#toc4_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[IV.1 Introduction](#toc4_1_)\n",
    "\n",
    "\n",
    "There is a need for a method that compensates the defect that there is no direct mapping between Thymio's sensors and the values which are used for control. The absolute position of the robot can be obtained directly from the camera and global path planning techniques, but the robot may have low precision and update rate, causing drifts when the velocity or acceleration is integrated. We can address this challenge by implementing Kalman filter as it provides a sound framework to use all available measurements in an optimal way. It relies on a model for the system structure and the effect of disturbances both on the state and on the measurements.\n",
    "\n",
    "The Kalman filter we implemented estimates the position and speed of the robot when it moves along the planned path. Specifically, we implemented extended Kalman filter to take into account the system involves nonlinear dynamics that would need to be linearized around the current estimate, focusing on local optimality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[IV.2 Methodology](#toc4_2_)\n",
    "\n",
    "Prior to the designing of our filter algorithm, we began with estimating Thymio's speed by let it cross seven blocks that are 50 mm each with a sample rate of 0.1 seconds and marked peaks of the ground sensor measurement indicating that it has crossed every block. This was to obtain velocity parameters, in which the method is identical to one of the activities found in Exercise 8 of the course. With known and obtained information, we could obtain the conversion factor of 0.4 to convert Thymio's speed to mm/s.\n",
    "\n",
    "![Find Peaks](report-images/find_peaks.png \"Fine Peaks\")\n",
    "\n",
    "Now, to obtain speed variance, we observed the data where the robot was moving by plotting the robot's speed and computed average velocity and thus the speed variance by diving the average speed with the robot's speed in mm/s.\n",
    "\n",
    "![Measured Vel](report-images/measured_vel.png \"Measured Vel\")\n",
    "\n",
    "With the assumption that half of the variance is cause by the measurement and the other half is caused by the perturbation to the states, we obtain \n",
    "\n",
    "$q_v = r_v = 16.08621$ $mm^2/s^2$ $\\space$ where $\\space$ $\\sigma^2_{vel} = 32.17242$ $mm^2/s^2$\n",
    "\n",
    "Variances on position state and measurement as well as those for the angle for orientation ($\\sigma^2_{pos}$ and $\\sigma^2_{\\theta}$) were an arbitrary value that were tuned manually for the specific Thymio robot that is going to be used for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[IV.3 Dynamics of Thymio](#toc4_3_)\n",
    "\n",
    "To begin with the implementation, the state space model is\n",
    "\n",
    "$\\hat{x}_{k-1|k-1} =  \\begin{bmatrix}\n",
    "                        x_{pos_{k-1}} \\\\\n",
    "                        y_{pos_{k-1}} \\\\\n",
    "                        \\theta_{k-1} \\\\\n",
    "                        v_{left_{k-1}} \\\\\n",
    "                        v_{right_{k-1}} \\\\\n",
    "                        \\end{bmatrix}$\n",
    "\n",
    "Under control input vector,\n",
    "\n",
    "$ u =[v_{trans}, v_{rot}] $, $\\space$ where $\\space$ $v_{tran} = \\frac{v_{left} + v_{right}}{2} $ $\\space$ and $\\space$ $ v_{rot} = \\frac{v_{left} - v_{right}}{l_{base}} $\n",
    "\n",
    "<img src=\"report-images/Thymio_odometry.png\" width=\"571\"/>\n",
    "\n",
    "Then, we can see that the robot's new position and orientation are predicted in the following way\n",
    "\n",
    "$x_{pos_{k}} = x_{pos_{k-1}} + v_{trans_{k}} \\cdot \\cos(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$y_{pos_{k}} = y_{pos_{k-1}} + v_{trans_{k}} \\cdot \\sin(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$\\theta_{k} = \\theta_{k-1} + v_{rot_{k}} \\cdot dk$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[IV.4 Design and Implementation of Filter](#toc4_4_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the steps of the extended Kalman filter we implemented, we would like to disclaim that the notations and formulas were referenced from lecture slides 8 of the Mobile Robotics course and online tutorial at [Extended Kalman Filter (EKF) with Python Code Example](https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/).\n",
    "\n",
    "We have introduced that the extended Kalman filter takes into account nonlinear system dynamics in the previous sections. In order to make prediction of the next state, We use a nonlinear motion model $f(x, u)$. That is, we can formulate their relation as:\n",
    "\n",
    "$\\hat{x}_{k|k-1} = f(\\hat{x}_{k-1|k-1}, u_k)$\n",
    "\n",
    "Our $f(x, u)$ is then introduced as\n",
    "\n",
    "$f(\\hat{x}_{k-1|k-1}, u_k) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & 0 & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this prediction step of the filter, we make a use of the predicted covariance matrix $P_{k-1|k-1}$.\n",
    "\n",
    "$P_{k-1|k-1} = \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as each column corresponds to the state variables, from the first column to the last column, being $x_{pos_{k-1}}$, $y_{pos_{k_1}}$, $\\theta_{k-1} $, $v_{left_{k-1}}$, and $v_{right_{k-1}}$, respectively.\n",
    "\n",
    "The updated (current) covariance matrix is as follows:\n",
    "\n",
    "$P_{k|k-1} = F_{k}P_{k-1|k-1}F_{k}^{T} + Q_{k}$\n",
    "\n",
    "where $F_{k}$ is the Jacobian of $f_{k}$ matrix, and $Q_{k}$ is the noise covariance matrix of the state model. The $Q_{k}$ matrix specifically represents the degree or the extent of how much the actual motion of Thymio deviates from its assumed state space model which would be the path that it should follow for our case.\n",
    "\n",
    "We can write the two matrices as below:\n",
    "\n",
    "$F_{k|k-1} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -v_{trans} \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & v_{trans} \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Q_{k|k-1} = \\alpha \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\alpha$ is a scaling factor, which we have initialized as $\\alpha =10$, such that \n",
    "\n",
    "$Q_{k-1|k-1} = 10 \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "We set $\\alpha$ large enough so that the filter tracks large changes in the wheel sensor measurements more closely as if it was smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class ***KalmanFilterExtended*** and the class is called in the **main.py** file where all the five components of the project are integrated.\n",
    "\n",
    "The function ***run_ekf()*** ensures that the extened filter is intiated and the snippet from **main.py** below ensures that the filter is running in a loop throughout the entire mission of the robot.\n",
    "\n",
    "Additionally, we have set parameters that defines *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. Specifically, we have set the threshold for orientation and position to be 35cm and 60°, and exceeding either (or both) of these values would be considered getting kidnaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(self, dt):\n",
    "        f, F = self.compute_fnF(dt)\n",
    "        self.x = f @ self.x\n",
    "        self.P = F @ self.P @ F.T + self.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, position, u, cam):\n",
    "        if cam:\n",
    "            R = self.R_cam\n",
    "            H = self.H_cam\n",
    "            z = np.array([position[0], position[1], position[2], u[0], u[1]])\n",
    "        else:\n",
    "            R = self.R_nocam\n",
    "            H = self.H_nocam\n",
    "            z = np.array([u[0], u[1]])\n",
    "\n",
    "        z_pred = H @ self.x\n",
    "        y = z - z_pred\n",
    "\n",
    "        S = H @ self.P @ H.T + R\n",
    "        K = self.P @ H.T @ np.linalg.pinv(S)\n",
    "        self.x += K @ y\n",
    "        self.P = self.P - K @ H @ self.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class `KalmanFilterExtended` and the class is called in the `main.py` file as\n",
    "```python\n",
    "    ekf = KalmanFilterExtended(np.array([thymio_pos_x * -1, thymio_pos_y, ((thymio_theta + 180) % 360) * np.pi / 180]), u)\n",
    "```\n",
    "\n",
    "The function `run_EKF` ensures that the extened filter is intiated the filter steps are correctly called in the correct order. The code snippet below is part of it.\n",
    "\n",
    "```python\n",
    "    ekf.prediction(dt)\n",
    "    measured_state = ekf.get_state()\n",
    "    ekf.update([pos_x, pos_y, theta], u, cam)\n",
    "    measurement_update = ekf.get_state()\n",
    "``` \n",
    "\n",
    "It helps us to have the filter run in `main.py` in a loop throughout the entire mission of the robot as\n",
    "\n",
    "```python\n",
    "    measured_state, kidnap = run_EKF(ekf, thymio_pos_x * -1, thymio_pos_y, theta, u, cam=camera)\n",
    "```\n",
    "\n",
    "Additionally, we have set parameters that define *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. This mode is apart from the *kidnapping* condition from the Computer Vision side, and we decided to have two modes of kidnap detection to account for a variety of possible ways to kidnap the robot. Specifically, we have set the threshold for orientation and position to be 100 mm and 1 rad, and exceeding either (or both) of these values would be considered getting kidnaped.\n",
    "\n",
    "```python\n",
    "    if dpos > 100 or dtheta > 1:\n",
    "        kidnap = True\n",
    "        print(\"Thymio is being kidnapped (moved too far or rotated too much)\")\n",
    "        cur_t = time.time()\n",
    "        ekf.count_time(cur_t)\n",
    "        ekf.get_state()\n",
    "    else:\n",
    "        kidnap = False\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also added a `draw_confidence_ellipse()` function that draws confidence ellipse taking from the first two diagonal components of the $P_{k}$ matrix as a means to visualize the uncertainty that the system has regarding the robot's pose.\n",
    "```python\n",
    "    P = get_cov().[:2, :2]\n",
    "```\n",
    "\n",
    "The method of implementing it was referenced from Github tutorial on [Robot Motion](https://github.com/jotaraul/jupyter-notebooks-for-robotics-courses/blob/master/3-Robot%20motion.ipynb) and [The Kalman Filter](https://engineeringmedia.com/controlblog/the-kalman-filter)\n",
    "\n",
    "Then the extracted $P_{k}$ matrix will be\n",
    "\n",
    "$P_{k} = \\begin{bmatrix}\n",
    "\\sigma_{xx}^2 & \\sigma_{xy}^2 \\\\ \n",
    "\\sigma_{yx}^2 & \\sigma_{yy}^2 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We computed the eigenvalues and eigenvectors of it as the major and minor axis of the confidence ellipse is determined by the eigenvalues and the angles of the confidence ellipse is determined by the eigenvectors.\n",
    "\n",
    "```python\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(P)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "``` \n",
    "\n",
    "Then, we used ChatGPT to add chi-squared confidence interval to the ellipse computing function to account for the nonlinearity of the system.\n",
    "\n",
    "```python\n",
    "    chi_squared_val = np.sqrt(5.991) \n",
    "    amplification = 1\n",
    "    major_axis = chi_squared_val * np.sqrt(eigenvalues[0]) * amplification\n",
    "    minor_axis = chi_squared_val * np.sqrt(eigenvalues[1]) * amplification\n",
    "\n",
    "    center = (int(mean[0]), int(mean[1]))\n",
    "    axes = (int(major_axis), int(minor_axis))\n",
    "    cv2.ellipse(frame, center, axes, angle, 0, 360, color, 2)\n",
    "``` \n",
    "In `main.py`, it is called as\n",
    "```python\n",
    "    draw_confidence_ellipse(frame_aruco_and_corners, ekf.get_cov()[:2, :2], [x, y], color=(0, 255, 255))\n",
    "```\n",
    "in the loop of updating every measurement so that the ellipse is also plotted dynamically.\n",
    "This visualization of positional uncertainty in 2D space provides better intuition of how the uncertainty changes during the mission and different modes such as kidnapping and switching between vision and no vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_5_'></a>[IV.5 Simulation](#toc5_5_)\n",
    "\n",
    "The simulation on filtering was done several times on the path planning scenario found in the previous section of Global Navigation. One of the examples is shown below, where the `run_EKF` receives a set $x$ and $y$ coordinate of the planned path in each iteration and applies filtering from the `KalmanFilterExtended` class functions. The code snippet of this can be found in `KFsim.py`. The purpose of this simulation is to observe whether the filtering is successfully running throughout the entire travel sequence and to observe different results in two different conditions: when Thymio has vision and when it does not. The simulation is done by imposing random noises of normal distribution on the wheel sensors and camera vision and compare the path the robot would take with the true path the path planning provides. When the simulation confirms that the filtering is operating well, we could integrate the algorithm into our `main.py`.\n",
    "\n",
    "<img src=\"report-images/EKF_simulation.png\" width=\"571\"/>\n",
    "<img src=\"report-images/kalman_simu.png\" width=\"571\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Conclusion](#toc6_)\n",
    "It has been an incredible journey to embark on this project within the constrained timeline of just three weeks, replete with challenges and invaluable learning opportunities. From the very beginning, we have had to face numerous obstacles, such as technical errors and conceptual hurdles, which have pushed us to refine our problem-solving skills and deepen our understanding of mobile robotics. Despite these challenges, our team has been able to overcome setbacks and achieve results we are proud of through persistence, discipline, and a collaborative spirit.\n",
    "\n",
    "Working through the complexities of the project was as rewarding as it was demanding. From understanding the theoretical basis to the implementation of practical solutions, each step provided an opportunity to reinforce and showcase our grasp of the principles of mobile robotics. The hands-on nature of the project tested not only our knowledge but also helped us grow as engineers, fostering resilience, adaptability, and creativity in tackling real-world problems.\n",
    "\n",
    "We would like to express our deep gratitude to everyone who guided and supported us through this journey. We would like to thank Professor Mondada, who inspired us with his experience and vision, and the teaching assistants, who were always ready to help, explain doubts, and give us valuable insights. Their encouragement and mentorship played a very important role in driving the success of our project.\n",
    "\n",
    "It is not only an academic milestone but also a source of personal and professional growth, and we will be carrying the lessons learned herein as we progress in our future endeavors related to robotics and beyond."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
