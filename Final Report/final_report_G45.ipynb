{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**MICRO-452 Final Report**](#toc0_)\n",
    "### <a id='toc1_1_1_'></a>[*Group 45: Anton Balykov, Teo Halevi, Cyprien Lacassagne, Seonmin Park*](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Table of Contents](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Project Objective](#toc0_)    \n",
    "- [I. Computer Vision](#toc1_)    \n",
    "  - [I.1 Introduction](#toc1_1_)    \n",
    "  - [I.2 Camera Set Up](#toc1_2_)    \n",
    "  - [I.3 Global Obstables and Thymio Detection](#toc1_3_)    \n",
    "- [II. Global Navigation](#toc2_)    \n",
    "  - [II.1 Introduction](#toc2_1_)    \n",
    "  - [II.2 Path Planning](#toc2_2_)    \n",
    "  - [II.3 Algorithm](#toc2_3_)    \n",
    "- [III. Local Navigation](#toc3_)    \n",
    "  - [III.1 Introduction](#toc3_1_)    \n",
    "  - [III.1 Local Avoidance](#toc3_2_)    \n",
    "- [IV. Filtering](#toc4_)    \n",
    "  - [IV.1 Introduction](#toc4_1_)    \n",
    "  - [IV.2 Methodology](#toc4_2_)    \n",
    "  - [IV.3 Dynamics of Thymio](#toc4_3_)    \n",
    "  - [IV.4 Design and Implementation of Filter](#toc4_4_)    \n",
    "  - [IV.5 Simulation](#toc4_5_)    \n",
    "- [Conclusion](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc0_'></a>[Project Objective](#toc0_)\n",
    "\n",
    "This project leverages the Thymio robot to explore and integrate key concepts introduced in the Introduction to Mobile Robotics course. We hereby focus on implementing and demonstrating core components: computer vision, global navigation, motion control, local navigation, and filtering. The mission of the robot is to navigate itself from the designated start position to the specified goal position, successfully avoiding fixed obstacles as well as dynamically placed one. Achieving this requires precise motion control, robust obstable detection, and efficient filtering technique. Such practical application of the five above-mentioned elements would reflect our team's comprehensive understanding of mobile robotics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[I. Computer Vision](#toc1_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[I.1 Introduction](#toc1_1_)\n",
    "\n",
    "Computer vision is the first step in our project, as it is the basic input. To steer the robot in an environment, we need to know the robot's position in that environment, what obstacles are present, where to go, and whether data acquisition is proceeding correctly. This is the purpose of the `ComputerVision` class we have implemented. All this data will then be used to predict the optimal path from the starting position to the goal, avoiding obstacles, and to provide an initial estimate of the position to be given to the Kalmann filter.\n",
    "\n",
    "We distinguish several parts in this explanation: the general setup, including the camera, the creation of the map, and the creation of markers to detect the robot and the goal. We then talk about detecting obstacles in the map, which the robot must avoid. Finally, the detection of the robot and the goal is a crucial part, which will be done using ArUco markers. \n",
    "\n",
    "## <a id='toc1_2_'></a>[I.2 Setup : camera, map, and markers](#toc1_2_)\n",
    "\n",
    "The first thing to create is the environment in which our robot will evolve. This environment is made up of delimited obstacles in a free zone for the robot. After several computer vision tests, we decided that our obstacles would be represented by red shapes on a white background. This choice is explained in greater detail in the following section on obstacle detection. Once our map has been created, we need to find a way of finding the robot in the image. To do this, we chose to use the ArUco markers provided by the OpenCV library in Python. These markers are QR-code-like representations with unique identifiers that can be easily detected in space. By defining a marker for the robot, and a marker for the goal, we can extract their respective positions to define the robot's shortest path to the goal.\n",
    "\n",
    "ArUco markers are defined by two things: a dictionary and an identifier. In our case, we use the dictionary `DICT_6X6_250` and IDs 4 and 8 for the robot and goal respectively. A marker is generated as follows, where `marker_id` is the defined ID of the marker, and `marker_size` is the size of the marker in pixels:\n",
    "```python\n",
    "dictionary   = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)\n",
    "marker_image = cv2.aruco.generateImageMarker(dictionary, marker_id, marker_size)\n",
    "```\n",
    "\n",
    "To have enough markers to test the robustness of our code, we created and printed a grid of 9 markers, for IDs from 0 to 8.\n",
    "\n",
    "<img src=\"report-images/aruco_grid.png\" alt=\"aruco_grid\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "The webcam used for our project has a resolution of 1920 by 1080 pixels. The camera is positioned above and in the center of the card, to minimize image distortion. An example of the setup with map, obstacles, robot, robot and goal markers and camera is shown below.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"Original frame from the webcam\" width=\"750\" class=\"center\"/>\n",
    "\n",
    "When using a camera, two parameters need to be determined: the camera matrix $C$ and the distortion coefficients $D$. Matrix $C$ contains the camera's intrinsic parameters: focal lengths and optical center, along the $x$ and $y$ axes. These parameters are  denoted by: $f_x$, $f_y$ (focal lengths) and $c_x$, $c_y$ (optical center), which gives us: $$C = \\begin{pmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "The distorsion matrix $D$ is defined as $D = \\begin{pmatrix} k_1 & k_2 & p_1 & p_2 & k_3 \\end{pmatrix}$ where the parameters are defined in [Camera Calibration](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html).\n",
    "\n",
    "Thw two matrices $C$ and $D$ are found using calibration. This step is usually performed using a reference image such as a checkerboard. We used a checkerboard with 9 by 7 inner corners, and with $20$mm square length. By taking several photos from different angles and heights of the checkerboard, we can correct the distorted image. An image of the calibration process is shown below.\n",
    "\n",
    "<img src=\"report-images/checkerboard_calibration.png\" alt=\"checkerboard_calibration.\" width=\"750\" class=\"center\"/>\n",
    "\n",
    "For our purposes, we have decided to set the origin at the top left corner of the image. To achieve this, the optical centers $c_x$ and $c_y$ were set to $0$. In addition, distortion coefficients were considered negligible when comparing data with and without calibration. Once these matrices are known, the camera is initialized and can be used to detect obstacles and ArUco markers.\n",
    "\n",
    "## <a id='toc1_3_'></a>[I.3 Detection of global obstacles](#toc1_3)\n",
    "\n",
    "To be able to steer the robot along the shortest path, we need to know its environment. This means knowing the location of global obstacles on the map.\n",
    "After several tests, we decided to create our obstacles using flat red shapes. This decision is part of our global obstacle detection strategy.\n",
    "\n",
    "To detect obstacles in the image, the first step is to apply a mask to the image to extract only the red areas. To do this, we work in the HSV domain of the image. We have determined that the different shades of red can belong to two intervals defined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_red_1 = np.array([0, 100, 100])\n",
    "upper_red_1 = np.array([10, 255, 255])\n",
    "\n",
    "lower_red_2 = np.array([170, 100, 100])\n",
    "upper_red_2 = np.array([180, 255, 255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function `apply_color_mask(frame, mask_color='n')` creates the image's red masks for each of the two intervals using the `cv2.inRange` verification function. To bring the masks together, we compare the images pixel by pixel using the OR operator. This returns an image with values of $255$ (white) for pixels detected as red, and $0$ (black) everywhere else.  Finally, the mask undergoes the morphological operation `cv2.MORPH_CLOSE` to reunite disjointed red areas or complete black pixels present in white areas.\n",
    "\n",
    "The result of this red mask is shown below. We recall the original image. Note that only the red shapes are shown in the black and white image.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"original_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "<img src=\"report-images/red_masked_frame.jpg\" alt=\"red_masked_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "This implementation allows us to assume that all obstacles in the image are red. This will facilitate their detection and enable ideal robot \n",
    "control.\n",
    "\n",
    "Once the red shapes have been extracted, we are interested in extracting their edges. Indeed, this is the only important information about an obstacle, since the obstacle's edge must not be exceeded. To extract the edges, we use a Canny edge detector implemented in OpenCV:\n",
    "```python\n",
    "cv2.Canny(red_mask, 50, 150)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/red_edges_frame.jpg\" alt=\"red_edges_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "The Canny edge detector involves several stages: noise reduction using Gaussian blurring, gradient calculation, non-maximum suppression, and edge tracking by hysteresis. The second and third arguments in `cv2.Canny` represent the lower and upper thresholds for the hysteresis step. Pixels with gradient intensities above the upper threshold are classified as edges, while those below the lower threshold are ignored. Pixels with intensities between the thresholds are included as edges only if they are connected to a strong edge. The limit values found were obtained after several tests to extract edges on different shades of red, and appear to be optimal for our application.\n",
    "\n",
    "To predict the shortest path between the robot and the goal, we use a visibility graph. To use it, we need to know the corners of each obstacle. Our function `get_corners_and_shape_edges(edges)` use a polygonal approximation of the shapes found. This approximation allows each obstacle to be represented as a polygon, and therefore to have the corners of these polygons. To study each contour separately, we use `cv2.findContours`. \n",
    "```python\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "\n",
    "Each contour is then approximated as a polygon, with a certain tolerance. Since each shape is approximated by polygons, rounded shapes can also be defined as a list of corners. This is the case for the ellipse in the figures below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate the contour to a polygon\n",
    "epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "approx  = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "# Extract corner point from the approximated polygon\n",
    "corners = [[int(point[0][0]), int(point[0][1])] for point in approx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the list of corners returned by our function will have the form:\n",
    "```python\n",
    "[[[a1,b1], [a2, b2], [a3, b3]],                      # x and y coordinate of the first obstacle\n",
    " [[x1, y1], [x2, y2], [x3, y3], [x4, y4], [x5, y5]], # x and y coordinate of the second obstacle\n",
    " [[o1, p1], [o2, p2], [o3, p3], [o4, p4]]]           # x and y coordinate of the third obstacle\n",
    "```\n",
    "\n",
    "The display of detected contours and corners gives the following image.\n",
    "\n",
    "<img src=\"report-images/red_corners_obstacles_frame.jpg\" alt=\"red_corners_obstacles_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "And by displaying the corners of the red obstacles in green on the original camera image, we obtain the following visualization:\n",
    "\n",
    "<img src=\"report-images/original_frame_with_corners_obstacles.jpg\" alt=\"original_frame_with_corners_obstacles\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "Note that the above functions give the positions of obstacle corners in pixels. These positions will be converted to millimetres to maintain consistency in the project's units. This conversion is performed using ArUco markers.\n",
    "\n",
    "## <a id='toc1_4_'></a>[I.4 Detection of the robot and the goal](#toc1_4_)\n",
    "\n",
    "Global obstacle detection is now implemented. The map in which the robot is moving is therefore known, and obstacles are clearly positioned. The missing step in directing the robot is to know its position, as well as the position of its goal. \n",
    "\n",
    "As mentioned in the introduction, we use ArUco markers to identify the robot and the goal. For the following explanations, we'll use the image below with markers 7 (top left), 3 (top right), and 5 (bottom). The photo has been deliberately taken at an angle, with marker 5 blurred to demonstrate the robustness of marker detection.\n",
    "\n",
    "<img src=\"report-images/aruco_3markers.jpg\" alt=\"aruco_3markers\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "Basic marker detection is performed by the `cv2.aruco.detectMarkers` function, which takes as arguments the frame to be analyzed, the dictionary used for markers (in our case `DICT_6X6_250`), and initialized parameters. This function returns a list of marker corners, as well as the identifiers of recognized markers.\n",
    "\n",
    "In the ComputerVision class, the marker detection and marker drawing operation is performed by the following lines, in the function `detect_and_estimate_pose()` and gives the image below:\n",
    "```python\n",
    "corners, marker_ids, _ = cv2.aruco.detectMarkers(frame_with_markers, self.aruco_dict, parameters=self.aruco_params)\n",
    "cv2.aruco.drawDetectedMarkers(frame_with_markers, corners, marker_ids)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/aruco_3markers_with_id.jpg\" alt=\"aruco_3markers_with_id\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "We can see that the markers are detected correctly: they are surrounded by a green border, their ID is shown in blue, and fourth corner of each marker is shown in red.\n",
    "\n",
    "The positioning of the red corner in the image above gives an indication of the next step in the detection process. The corners of a marker are defined in the following order:\n",
    "1. Bottom left\n",
    "2. Bottom right\n",
    "3. Top right\n",
    "4. Top left (red corner shown)\n",
    "\n",
    "where the reference for top, bottom, left, right is given by the conventional reading of the text above the marker.\n",
    "\n",
    "To extract the position of markers in mm, use the function `cv2.aruco.estimatePoseSingleMarkers` as follows: \n",
    "```python\n",
    "rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners,\n",
    "                                                      self.ARUCO_MARKER_SIDE_LENGTH,\n",
    "                                                      self.mtx,\n",
    "                                                      self.dst)\n",
    "```\n",
    "\n",
    "The `self.ARUCO_MARKER_SIDE_LENGTH` parameter defined in class initialization is the length of a marker side in mm. It is used to switch from pixel coordinates to mm coordinates. Note also that the `mtx` and `dst` matrices are used, corresponding respectively to the $C$ and $D$ matrices defined in the calibration explained above.\n",
    "\n",
    "All this is done by calling the function:\n",
    "```python\n",
    "frame_with_markers, marker_ids, rvecs, tvecs, aruco_diagonal_pixels = vision.detect_and_estimate_pose(frame_marker)\n",
    "```\n",
    "\n",
    "The result contains two variables `rvecs` (rotation vectors) and `tvecs` (translation vectors).\n",
    "The rotation vector (`rvecs`) is first converted into a rotation matrix, which provides a representation of 3D orientation. A $4\\times 4$ identity matrix is initialized to serve as a transformation matrix, with its top-left $3\\times 3$ portion allocated for the rotation information. The function `cv2.Rodrigues()` is then used to convert the rotation vector into a $3\\times 3$ rotation matrix.\n",
    "\n",
    "Next, the $3\\times 3$ rotation matrix is converted into a quaternion using the `scipy.spatial.transform.Rotation.from_matrix()` method. A quaternion is a four-dimensional representation $[x, y, z, w]$ of rotation. In the quaternion, $x$, $y$, and $z$ represent the vector part, while $w$ represents the scalar part. Finally, the quaternion is converted to Euler angles using the function `euler_from_quaternion`. This function calculates roll (rotation around the $x$-axis), pitch (rotation around the $y$-axis), and yaw (rotation around the $z$-axis). Among these, the yaw angle is extracted because it represents the rotation of the marker in the $x$-$y$ plane, which corresponds to the marker's orientation relative to the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_marker_pose(self, frame, marker_ids, rvecs, tvecs):\n",
    "    \"\"\"\n",
    "    Process pose information for each detected marker and draw on a copy of the original frame.\n",
    "\n",
    "    original_frame: The frame to process\n",
    "    marker_ids, rvecs, tvecs: Detected marker details (IDs, rotation vectors, translation vectors)\n",
    "    mtx, dst: Camera matrix and distortion coefficients\n",
    "\n",
    "    Returns:\n",
    "        frame_with_vectors: A new frame with the vectors drawn\n",
    "        markers_data: Data of the markers (x-position, y-position, yaw-angle for each marker)\n",
    "    \"\"\"\n",
    "\n",
    "    frame_with_vectors = frame.copy()\n",
    "    markers_data = []\n",
    "\n",
    "    for i, marker_id in enumerate(marker_ids):\n",
    "        transform_translation_x = 1000*(tvecs[i][0][0]) # 1000* to have in millimeters.\n",
    "        transform_translation_y = 1000*(tvecs[i][0][1])\n",
    "        transform_translation_z = 1000*tvecs[i][0][2]\n",
    "\n",
    "        rotation_matrix = np.eye(4)\n",
    "        rotation_matrix[0:3, 0:3] = cv2.Rodrigues(np.array(rvecs[i][0]))[0]\n",
    "        r = R.from_matrix(rotation_matrix[0:3, 0:3])\n",
    "        quat = r.as_quat()\n",
    "\n",
    "        roll_x, pitch_y, yaw_z = self.euler_from_quaternion(quat[0], quat[1], quat[2], quat[3])\n",
    "\n",
    "        roll_x = math.degrees(roll_x)\n",
    "        pitch_y = math.degrees(pitch_y)\n",
    "        yaw_z = -(math.degrees(yaw_z)-90)\n",
    "\n",
    "        markers_data.extend([[marker_id[0], transform_translation_x, transform_translation_y, yaw_z]])\n",
    "\n",
    "        cv2.drawFrameAxes(frame_with_vectors, self.mtx, self.dst, rvecs[i], tvecs[i], 0.05)\n",
    "\n",
    "    return frame_with_vectors, markers_data\n",
    "\n",
    "\n",
    "def euler_from_quaternion(x, y, z, w):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "    roll is rotation around x in radians (counterclockwise)\n",
    "    pitch is rotation around y in radians (counterclockwise)\n",
    "    yaw is rotation around z in radians (counterclockwise)\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    roll_x = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = np.clip(t2, -1.0, 1.0)\n",
    "    pitch_y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "    yaw_z = math.atan2(t3, t4)\n",
    "\n",
    "    return roll_x, pitch_y, yaw_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the function `process_marker_pose` defined above, we obtain the frame with the detected vectors, as well as the coordinates of each marker.\n",
    "\n",
    "```python\n",
    "frame_with_vectors, markers_data = vision.process_marker_pose(frame_with_markers, marker_ids, rvecs, tvecs)\n",
    "```\n",
    "\n",
    "<img src=\"report-images/aruco_3markers_with_vectors.jpg\" alt=\"aruco_3markers_with_vectors\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "For this image, the coordinates are as follows, with ID, $x$-coordinate (mm), $y$-coordinate (mm) and yaw-angle (degrees) for each marker.\n",
    "\n",
    "```python\n",
    "[[5, 166.79759785976782, 139.66100169531774, 97.87032032706453],\n",
    " [7, 148.05928528540807, 66.24491386374977, 120.86243771133647],\n",
    " [3, 296.46820843077455, 67.33202952024861, -45.05418817033191]]\n",
    "```\n",
    "\n",
    "The yaw angle is defined as 0 when the green vector is vertical, and follows the clockwise direction. $x$ increases when the marker goes to the right of the image, and $y$ increases when the marker goes to the bottom of the image.\n",
    "\n",
    "Returning to the original image with the Thymio, we can now display all the information: obstacles, obstacle corners, robot and goal.\n",
    "\n",
    "<img src=\"report-images/original_frame_from_webcam.jpg\" alt=\"original_frame\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "<img src=\"report-images/frame_aruco_and_corners.jpg\" alt=\"frame_aruco_and_corners\" width=\"500\" class=\"center\"/>\n",
    "\n",
    "\n",
    "**NOTE: About conversion from pixels to mm**\n",
    "\n",
    "ArUco marker coordinates are given directly in mm by the function `detect_and_estimate_pose()`.\n",
    "On the other hand, the coordinates of obstacle corners are given in pixels.\n",
    "\n",
    "To have mm everywhere and ensure consistency, we need to convert the coordinates of the obstacle corners into mm. This is done by finding the pixel-mm conversion factor. To do this, we need a length that is both known in mm and known in pixels. We use the information contained in the ArUco markers. In fact, the corners list used in the `detect_and_estimate_pose()` function contains the pixel coordinates of the corners of each marker. Taking the first marker detected, we can calculate the length of its diagonal in pixels. With the actual length of the diagonal in mm known, the conversion factor is found and used to convert the coordinates of the obstacle corners into mm:\n",
    "\n",
    "```python\n",
    "aruco_diagonal_pixels = math.sqrt((corners[0][0][2][1] - corners[0][0][0][1])**2 + (corners[0][0][2][0] - corners[0][0][0][0])**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[I.5 Detection of kidnapping](#toc1_5_)\n",
    "\n",
    "The last part for the `ComputerVision` class is to detect whether the camera has been covered, or whether the robot has been kidnapped and repositioned elsewhere. We rely on the detection of the marker assigned to the robot to create the corresponding function. If the marker assigned to the robot does not belong to the list of detected markers, then the function returns `True` (to say that the robot has been kidnapped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camera_covered(self, detected_markers):\n",
    "    \"\"\"\n",
    "    Returns True (kidnapped) if the ARUCO_ROBOT_ID is not detected, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    if detected_markers is None:\n",
    "        return True\n",
    "    else:\n",
    "        return self.ARUCO_ROBOT_ID not in detected_markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[II. Global Navigation](#toc2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[II.1 Introduction](#toc2_1_)\n",
    "\n",
    "One main component of this project is navigation. Here, we discuss about our implementation of the global navigation, whose main objective is to find a collision-free path from a source to a goal, on a map filled with polygons acting as obtacles. The environment of motion is imply a 2D flat surface such as the ground or a table. The global, and thus **persistant** obstacles are red polygons made of paper, and placed freely by hand to test the robustness of the path planning algorithm. The boundaries of our map are determined by the field of view - and thus the height - of the camera from the ground, as regions that lie outside the visible field are not processed. To find the optimal path from the robot to the goal, the strategy relies on a metric map and visibility graph, as well as Dijkstra's graph search algorithm.\n",
    "\n",
    "Global navigation is only involved in the beginning of the program, outside of the main loop. The only case where global path is updated is when the robot is \"kidnapped\", i.e. moved by hand to another position on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[II.2 Initial Strategy](#toc2_2_)\n",
    "\n",
    "Initially, we decided to go with a discretized fixed-size cell-map and use Dijktra's search algorithm. Although the implementation was not difficult, we thought that it was not as elegant as the more intuitive visibility graph approach. Nonetheless, here is the result of this preliminary work:\n",
    "\n",
    "<img src=\"report-images/cell_decomposition.png\" alt=\"Cell_decomposition\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[II.3 Polygon Expansion](#toc2_3_)\n",
    "\n",
    "We decided to clearly separate features detection (`ComputerVision`) and path planning (`Navigation`).\n",
    "The class `Navigation` is constructed by receiving the position of the robot (source), of the goal, and of the vertices of each obstacle. All of this data is extracted from the camera image by the class `ComputerVision`, as explained in Part I. Each obstacle conists of a list of points in the form `[x, y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvisgraph as vg\n",
    "\n",
    "THYMIO_RADIUS = 70 # mm\n",
    "\n",
    "class Navigation:\n",
    "    def __init__(self, obstacles, robot, goal):\n",
    "        \"\"\"\n",
    "        :param obstacles: A list of all the obstacles on the map. An obstacle is a list of \n",
    "                            vertices, ordered CCW.\n",
    "        :param robot: The position of the robot.\n",
    "        :param goal: The position of the goal.\n",
    "        \"\"\"\n",
    "        self.obstacles = obstacles\n",
    "        self.obstacles_count = len(obstacles)\n",
    "        self.extended_obstacles = []\n",
    "        self.source = robot\n",
    "        self.goal = goal\n",
    "        self.path = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into graph creation, the polygons need to be expanded in order to account for the size of the robot. Otherwise, each obstacle's vertex would be registered as a node, and thus a potential waypoint in the final path, and would be mapped to the center of the robot. This is not ideal since the robot would bump into the obstacles. The function `augment_obstacles` loops through the polygons and computes two new points for each vertex, expanded once in the direction of each of the edges that make up the vertex, and once in the direction of their normals, by a constant factor `THYMIO_RADIUS`, which is the radius of the robot in mm. That way, we ensure the robot always keeps a safe minimal distance with the obstacles, even when some portion of the resulting path is tengential to a polygon edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def augment_obtacles(self):\n",
    "    \"\"\"\n",
    "    Augments the countour of the obstacles w.r.t. the radius of the robot.\n",
    "    \"\"\"\n",
    "    for polygon in self.obstacles:\n",
    "        extended_polygon = []\n",
    "\n",
    "        count = len(polygon)\n",
    "        for i in range(count):\n",
    "            vertex = polygon[i]\n",
    "            prev = polygon[i-1]\n",
    "            next = (polygon[i+1] if i < count-1 else polygon[0])\n",
    "\n",
    "            edge1 = np.subtract(vertex, prev)\n",
    "            edge2 = np.subtract(next,vertex)\n",
    "\n",
    "            dir1 = np.array(edge1 / np.linalg.norm(edge1))\n",
    "            dir2 = np.array(edge2 / np.linalg.norm(edge2))\n",
    "\n",
    "            perp1 = -np.array([dir1[1], -dir1[0]])\n",
    "            perp2 = -np.array([dir2[1], -dir2[0]])\n",
    "\n",
    "            v1 = vertex + THYMIO_RADIUS * dir1 + THYMIO_RADIUS * perp1\n",
    "            v2 = vertex - THYMIO_RADIUS * dir2 + THYMIO_RADIUS * perp2\n",
    "\n",
    "            # Preserve the winding of the original polygon\n",
    "            angle = np.arccos(np.clip(np.dot(-dir1, dir2), -1.0, 1.0))\n",
    "            if angle <= np.pi / 2.0:\n",
    "                extended_polygon.append(v1)\n",
    "                extended_polygon.append(v2)\n",
    "            else:\n",
    "                extended_polygon.append(v2)\n",
    "                extended_polygon.append(v1)\n",
    "\n",
    "        self.extended_obstacles.append(extended_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result of our implemantation of polygon expansion. In this example, we set the vertices and thymio's radius to arbitrary values.\n",
    "\n",
    "<img src=\"report-images/Polygon_expansion.png\" alt=\"Polygon_expansion\" width=\"500\"/>\n",
    "\n",
    "Expanding the vertices by two points instead of only one, allows for a smoother bypassing and reduces the total path length, while increasing the accuracy of the Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[II.4 Visibility Graph & Graph Search](#toc2_4)\n",
    "\n",
    "Then, a visibility graph is built using the open-source library [pyvisgraph](https://github.com/TaipanRex/pyvisgraph). This library uses [Lee's algorithm](https://dav.ee/papers/Visibility_Graph_Algorithm.pdf), O(nÂ²log(n)), to compute the visibility of each node.<br>\n",
    "This operation will fail if the winding of the obstacles is incoherent. That is why we take care of expanding the corners of the obstacles in the right order.\n",
    "\n",
    "Finally, the shortest path is computed using Dijkstra's algorithm, and returned to the main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(self):\n",
    "    \"\"\"\n",
    "    Computes the shortest path from source (robot) to goal, while avoiding the obstacles.\n",
    "\n",
    "    :returns:\n",
    "        A list of the coordinates of each node from robot position to goal.\n",
    "    \"\"\"\n",
    "    self.augment_obtacles(self)\n",
    "\n",
    "    graph = vg.VisGraph()\n",
    "    polygons = []\n",
    "    for obstacle in self.extended_obstacles:\n",
    "        polygon = []\n",
    "        for point in obstacle:\n",
    "            polygon.append(vg.Point(point[0], point[1]))\n",
    "\n",
    "        polygons.append(polygon)\n",
    "            \n",
    "    graph.build(polygons)\n",
    "    path = graph.shortest_path(vg.Point(self.source[0], self.source[1]), vg.Point(self.goal[0], self.goal[1]))\n",
    "\n",
    "    for point in path:\n",
    "        self.path.append(point)\n",
    "        print(point)\n",
    "\n",
    "    return self.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='toc2_5_'></a>[II.5 Results](#toc2_5)\n",
    "\n",
    "Below are the results of two different configurations that shows how the path changes when the first shortest path (left) is blocked (right). A pass through between two obstacles is choked when at least one vertex of a polygon lies into the expanded version of the other one, as you can see on the right image, where we lowered the middle triangle by 50 units.\n",
    "\n",
    "<img src=\"report-images/path_planning_1.png\" alt=\"path_planning_1\" width=\"500\"/> <img src=\"report-images/path_planning_2.png\" alt=\"path_planning_2\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[III. Motion Control & Local Navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[III.1 Introduction](#toc0_)\n",
    "\n",
    "After extracting the information from the camera and defining the objective for the robot to go to, the next step is to apply motion control and local navigation to guide the robot to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc6_2_'></a>[III.2 Thymio Class](#toc0_)\n",
    "\n",
    "To read the readings from Thymio's sensors and control the robot's movement, the `Thymio` class was implemented.\n",
    "\n",
    "Main tasks of this class are:\n",
    "- Communicate with the robot via `ClientAsync` from [`tdmclient`](https://pypi.org/project/tdmclient/) library\n",
    "- Read the sensor values (horizontal proximity and wheels speed)\n",
    "- Control the robot's movement (set the speed of the wheels)\n",
    "\n",
    "The class consists of multiple constants and functions to achieve the above tasks.\n",
    "\n",
    "---\n",
    "### Constants\n",
    "1. Constants to store strings of the sensor names and motor names:\n",
    "    ```\n",
    "        SENSORS_HORIZONTAL = \"prox.horizontal\"\n",
    "        SPEED_LEFT = \"motor.left.speed\"\n",
    "        SPEED_RIGHT = \"motor.right.speed\"\n",
    "\n",
    "        MOTOR_LEFT = \"motor.left.target\"\n",
    "        MOTOR_RIGHT = \"motor.right.target\"\n",
    "    ```\n",
    "\n",
    "2. Constants to set the sensitivity of obstacle detection, reaching the goal and when to start turning towards the next target:\n",
    "    ```\n",
    "        GOAL_THRESHOLD = 10\n",
    "        OBSTACLE_THRESHOLD = 1800\n",
    "        ANGLE_THRESHOLD = 0.01\n",
    "    ```\n",
    "\n",
    "3. `W` matrix with the weights for the horizontal sensors during local navigation:\n",
    "    ```\n",
    "        SCALE = 0.03\n",
    "        W = np.array([[2, 1, -4, -1, -2], [-2, -1, -2, 1, 2]]) * SCALE\n",
    "    ```\n",
    "\n",
    "    The visualization of the weights is shown below (**from the Week 3 exercise**):\n",
    "    \n",
    "    <img src=\"./report-images/ANN_robot_control.png\" alt=\"Weights visualization\" width=\"500\"/>\n",
    "\n",
    "    Scaling factor is used to adjust the target speed of the robot.\n",
    "\n",
    "4. Speed of the robot:\n",
    "    ```\n",
    "        SPEED = 70\n",
    "    ```\n",
    "    Used to set the speed of the robot's wheels. Was chosen so the robot moves at a reasonable speed enough to turn and not too slow.\n",
    "\n",
    "5. A constant to account for a sharp turn:\n",
    "    ```\n",
    "        SHARP_TURN_THRESHOLD = np.pi / 6\n",
    "    ```\n",
    "    As the movement controller was chosen to move in straight lines or turn if there is an angle between the robot's orientation and the target, sometimes the angle and the radius of turn can be too big to reach the next target. THis constant is used in the movement controller to let robot turn in-place instead of more smooth turns. Set to 30 degrees so that robot moves more in straight lines and truns faster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The instance of the class is created in the `main.py` file to connect to the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.client = ClientAsync()\n",
    "    self.node = aw(self.client.wait_for_node())\n",
    "    aw(self.node.lock())\n",
    "\n",
    "    self.goal = None\n",
    "    self.position = None\n",
    "    self.orientation = None\n",
    "    self.is_on_goal = False\n",
    "    self.is_kidnapped = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function also sets the flags needed to define the state of the robot during the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The function to set the motor speeds of the Thymio (used when applying any movement to the robot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_motors(self, left_motor: int, right_motor: int, verbose: bool = False):\n",
    "    if verbose:\n",
    "        print(\"\\t\\t Setting speed : \", left_motor, right_motor)\n",
    "\n",
    "    aw(self.node.set_variables({\n",
    "        self.MOTOR_LEFT: [left_motor],\n",
    "        self.MOTOR_RIGHT: [right_motor]\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Functions to read sensor data (used to detect obstacles for local navigation and to estimate the robot's position for filtering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_horizontal_sensors(self, verbose: bool = False):\n",
    "    aw(self.client.wait_for_status(self.client.NODE_STATUS_READY))\n",
    "    aw(self.node.wait_for_variables({self.SENSORS_HORIZONTAL}))\n",
    "\n",
    "    values = self.node.var[self.SENSORS_HORIZONTAL]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\t\\t Sensor values : \", values)\n",
    "\n",
    "    return np.array(values)[:5]\n",
    "\n",
    "def get_wheels_speed(self, verbose: bool = False):\n",
    "    aw(self.client.wait_for_status(self.client.NODE_STATUS_READY))\n",
    "    aw(self.node.wait_for_variables({self.SPEED_LEFT, self.SPEED_RIGHT}))\n",
    "\n",
    "    left_speed = self.node.var[self.SPEED_LEFT]\n",
    "    right_speed = self.node.var[self.SPEED_RIGHT]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\t\\t Wheel speeds : \", left_speed, right_speed)\n",
    "\n",
    "    return np.array([left_speed[0], right_speed[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Setters of the position, orientation and goal of the robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_goal(self, goal):\n",
    "    self.goal = goal\n",
    "\n",
    "def set_position(self, position: np.ndarray):\n",
    "    self.position = position\n",
    "\n",
    "def set_orientation(self, orientation):\n",
    "    self.orientation = orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Functions to stop and correctly delete the instance of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(self):\n",
    "    self.set_motors(0, 0)\n",
    "\n",
    "def __del__(self):\n",
    "    aw(self.node.unlock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Functions to account for kidnapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kidnap(self):\n",
    "    self.is_kidnapped = True\n",
    "    self.stop()\n",
    "\n",
    "def recover(self):\n",
    "    self.is_kidnapped = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Function to apply the motion control to the robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_point(self, target: np.ndarray, verbose: bool = False):\n",
    "    path = target - self.position\n",
    "\n",
    "    dst = np.sqrt(np.sum(np.square(path)))\n",
    "\n",
    "    if dst <= self.GOAL_THRESHOLD:\n",
    "        if np.sqrt(np.sum(np.square(self.position - self.goal))) <= self.GOAL_THRESHOLD:\n",
    "            self.is_on_goal = True\n",
    "\n",
    "        return True\n",
    "\n",
    "    angle = self.orientation - np.arctan2(path[1], path[0])\n",
    "\n",
    "    if angle > np.pi:\n",
    "        angle = angle - 2 * np.pi\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Distance: \", dst)\n",
    "        print(\"Orientation: \", self.orientation)\n",
    "        print(\"Angle: \", angle)\n",
    "        print(\"Position: \", self.position)\n",
    "        print(\"Target: \", target)\n",
    "\n",
    "    if angle > self.ANGLE_THRESHOLD:\n",
    "        if angle > self.SHARP_TURN_THRESHOLD:\n",
    "            self.set_motors(self.SPEED, -self.SPEED)\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            self.set_motors(self.SPEED, -self.SPEED)\n",
    "    elif angle < -self.ANGLE_THRESHOLD:\n",
    "        if angle < -self.SHARP_TURN_THRESHOLD:\n",
    "            self.set_motors(-self.SPEED, self.SPEED)\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            self.set_motors(-self.SPEED, self.SPEED)\n",
    "\n",
    "    self.set_motors(self.SPEED, self.SPEED)\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `if dst <= self.GOAL_THRESHOLD` check is made in order to check if the target has been reached. There is an additional check for the goal to be reached in case the robot is already on the goal (in that case the `is_on_goal` flag is set to `True`).\n",
    "\n",
    "Here angle is computed as the difference between the orientation of the robot and the angle of the path to the target. If the angle is greater than the threshold, the robot turns towards the target. Otherwise, it moves forward.\n",
    "Additional check is made in order to have angles within the range of $[-\\pi, \\pi]$.\n",
    "In case of sharp turn, that is defined by the constant `SHARP_TURN_THRESHOLD` (described above), the robot turns in-place (done with the `time.sleep(0.5)` to make sure the robot has turned enough before moving forward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[III.3 Global Navigation Movement](#toc0_)\n",
    "\n",
    "Global navigation is implemented via following the points on the global path defined by the path-planning algorithm explained in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVEMENT\n",
    "target = global_path[0]\n",
    "if thymio.move_to_point(np.array([target.x * -1, target.y])):\n",
    "    print(f\"Reached checkpoint {check_num}\")\n",
    "    check_num += 1\n",
    "    global_path.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target is chosen as the next point on the global path. The robot moves towards the target point until it reaches it. Once the target is reached, the point is removed from the global path and the robot moves towards the next point.\n",
    "\n",
    "*The `target.x * -1` is used to flip the $x$-axis of the target point as the coordinate system of the camera is inverted (the `(0,0)` point is in the top-left corner and the $x$-axis looks to the left compared to the $y$-axis).\n",
    "\n",
    "<img src=\"./report-images/coordinate_system.jpg\" alt=\"Coordinate system of the camera\" width=\"500\"/>\n",
    "\n",
    "The inversion was done in order to have the standard Decart coordinate system and to not confuse the robot's movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[III.4 Local Navigation Movement](#toc0_)\n",
    "\n",
    "For the local navigation part the obstacle avoidance algorithm similar to the exercise of **Week 3** is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_navigation(self):\n",
    "     while np.any(self.get_horizontal_sensors() > self.OBSTACLE_THRESHOLD):\n",
    "        motor_values = self.W @ self.get_horizontal_sensors().T\n",
    "        left_motor = int(motor_values[0])\n",
    "        right_motor = int(motor_values[1])\n",
    "\n",
    "        self.set_motors(left_motor, right_motor)\n",
    "\n",
    "    self.set_motors(self.SPEED, self.SPEED)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the horizontal sensors of the robot detect an obstacle, the previously described `W` matrix of weights is multiplied by the sensor values and added to the speed of the robot. This way the robot is able to avoid the obstacles by driving around them. The `time.sleep(1)` is used to make sure that it takes some time for the robot to move around the obstacle and only after that continue with the global navigation.\n",
    "\n",
    "The algorithm is similar to the one mentioned in **Week 3 exercise**:\n",
    "\n",
    "<img src=\"./report-images/ANN_algorithm.png\" alt=\"Local Navigation algorithm\" width=\"600\"/>\n",
    "\n",
    "The local navigation routine is handled in the main loop of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL NAVIGATION\n",
    "if not thymio.is_kidnapped and np.any(thymio.get_horizontal_sensors() > thymio.OBSTACLE_THRESHOLD):\n",
    "    thymio.local_navigation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[IV. Filtering](#toc4_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[IV.1 Introduction](#toc4_1_)\n",
    "\n",
    "\n",
    "There is a need for a method that compensates the defect that there is no direct mapping between Thymio's sensors and the values which are used for control. The absolute position of the robot can be obtained directly from the camera and global path planning techniques, but the robot may have low precision and update rate, causing drifts when the velocity or acceleration is integrated. We can address this challenge by implementing Kalman filter as it provides a sound framework to use all available measurements in an optimal way. It relies on a model for the system structure and the effect of disturbances both on the state and on the measurements.\n",
    "\n",
    "The Kalman filter we implemented estimates the position and speed of the robot when it moves along the planned path. Specifically, we implemented extended Kalman filter to take into account the system involves nonlinear dynamics that would need to be linearized around the current estimate, focusing on local optimality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[IV.2 Methodology](#toc4_2_)\n",
    "\n",
    "Prior to the designing of our filter algorithm, we began with estimating Thymio's speed by let it cross seven blocks that are 50 mm each with a sample rate of 0.1 seconds and marked peaks of the ground sensor measurement indicating that it has crossed every block. This was to obtain velocity parameters, in which the method is identical to one of the activities found in Exercise 8 of the course. With known and obtained information, we could obtain the conversion factor of 0.4 to convert Thymio's speed to mm/s.\n",
    "\n",
    "<img src=\"report-images/find_peaks.png\" alt=\"Find Peaks\" width=\"500\"/>\n",
    "\n",
    "Now, to obtain speed variance, we observed the data where the robot was moving by plotting the robot's speed and computed average velocity and thus the speed variance by diving the average speed with the robot's speed in mm/s.\n",
    "\n",
    "<img src=\"report-images/measured_vel.png\" alt=\"Measured Vel\" width=\"500\"/>\n",
    "\n",
    "With the assumption that half of the variance is cause by the measurement and the other half is caused by the perturbation to the states, we obtain \n",
    "\n",
    "$q_v = r_v = 16.08621$ $mm^2/s^2$ $\\space$ where $\\space$ $\\sigma^2_{vel} = 32.17242$ $mm^2/s^2$\n",
    "\n",
    "Variances on position state and measurement as well as those for the angle for orientation ($\\sigma^2_{pos}$ and $\\sigma^2_{\\theta}$) were an arbitrary value that were tuned manually for the specific Thymio robot that is going to be used for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[IV.3 Dynamics of Thymio](#toc4_3_)\n",
    "\n",
    "To begin with the implementation, the state space model is\n",
    "\n",
    "$\\hat{x}_{k-1|k-1} =  \\begin{bmatrix}\n",
    "                        x_{pos_{k-1}} \\\\\n",
    "                        y_{pos_{k-1}} \\\\\n",
    "                        \\theta_{k-1} \\\\\n",
    "                        v_{left_{k-1}} \\\\\n",
    "                        v_{right_{k-1}} \\\\\n",
    "                        \\end{bmatrix}$\n",
    "\n",
    "Under control input vector,\n",
    "\n",
    "$ u =[v_{trans}, v_{rot}] $, $\\space$ where $\\space$ $v_{tran} = \\frac{v_{left} + v_{right}}{2} $ $\\space$ and $\\space$ $ v_{rot} = \\frac{v_{left} - v_{right}}{l_{base}} $\n",
    "\n",
    "<img src=\"report-images/Thymio_odometry.png\" width=\"500\"/>\n",
    "\n",
    "Then, we can see that the robot's new position and orientation are predicted in the following way\n",
    "\n",
    "$x_{pos_{k}} = x_{pos_{k-1}} + v_{trans_{k}} \\cdot \\cos(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$y_{pos_{k}} = y_{pos_{k-1}} + v_{trans_{k}} \\cdot \\sin(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$\\theta_{k} = \\theta_{k-1} + v_{rot_{k}} \\cdot dk$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[IV.4 Design and Implementation of Filter](#toc4_4_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the steps of the extended Kalman filter we implemented, we would like to disclaim that the notations and formulas were referenced from lecture slides 8 of the Mobile Robotics course and online tutorial at [Extended Kalman Filter (EKF) with Python Code Example](https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/).\n",
    "\n",
    "We have introduced that the extended Kalman filter takes into account nonlinear system dynamics in the previous sections. In order to make prediction of the next state, We use a nonlinear motion model $f(x, u)$. That is, we can formulate their relation as:\n",
    "\n",
    "$\\hat{x}_{k|k-1} = f(\\hat{x}_{k-1|k-1}, u_k)$\n",
    "\n",
    "Our $f(x, u)$ is then introduced as\n",
    "\n",
    "$f(\\hat{x}_{k-1|k-1}, u_k) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & 0 & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this prediction step of the filter, we make a use of the predicted covariance matrix $P_{k-1|k-1}$.\n",
    "\n",
    "$P_{k-1|k-1} = \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as each column corresponds to the state variables, from the first column to the last column, being $x_{pos_{k-1}}$, $y_{pos_{k_1}}$, $\\theta_{k-1} $, $v_{left_{k-1}}$, and $v_{right_{k-1}}$, respectively.\n",
    "\n",
    "The updated (current) covariance matrix is as follows:\n",
    "\n",
    "$P_{k|k-1} = F_{k}P_{k-1|k-1}F_{k}^{T} + Q_{k}$\n",
    "\n",
    "where $F_{k}$ is the Jacobian of $f_{k}$ matrix, and $Q_{k}$ is the noise covariance matrix of the state model. The $Q_{k}$ matrix specifically represents the degree or the extent of how much the actual motion of Thymio deviates from its assumed state space model which would be the path that it should follow for our case.\n",
    "\n",
    "We can write the two matrices as below:\n",
    "\n",
    "$F_{k|k-1} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -v_{trans} \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & v_{trans} \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Q_{k|k-1} = \\alpha \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\alpha$ is a scaling factor, which we have initialized as $\\alpha =10$, such that \n",
    "\n",
    "$Q_{k-1|k-1} = 10 \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "We set $\\alpha$ large enough so that the filter tracks large changes in the wheel sensor measurements more closely as if it was smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class ***KalmanFilterExtended*** and the class is called in the **main.py** file where all the five components of the project are integrated.\n",
    "\n",
    "The function ***run_ekf()*** ensures that the extened filter is intiated and the snippet from **main.py** below ensures that the filter is running in a loop throughout the entire mission of the robot.\n",
    "\n",
    "Additionally, we have set parameters that defines *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. Specifically, we have set the threshold for orientation and position to be 35cm and 60Â°, and exceeding either (or both) of these values would be considered getting kidnaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(self, dt):\n",
    "        f, F = self.compute_fnF(dt)\n",
    "        self.x = f @ self.x\n",
    "        self.P = F @ self.P @ F.T + self.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, position, u, cam):\n",
    "        if cam:\n",
    "            R = self.R_cam\n",
    "            H = self.H_cam\n",
    "            z = np.array([position[0], position[1], position[2], u[0], u[1]])\n",
    "        else:\n",
    "            R = self.R_nocam\n",
    "            H = self.H_nocam\n",
    "            z = np.array([u[0], u[1]])\n",
    "\n",
    "        z_pred = H @ self.x\n",
    "        y = z - z_pred\n",
    "\n",
    "        S = H @ self.P @ H.T + R\n",
    "        K = self.P @ H.T @ np.linalg.pinv(S)\n",
    "        self.x += K @ y\n",
    "        self.P = self.P - K @ H @ self.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class `KalmanFilterExtended` and the class is called in the `main.py` file as\n",
    "```python\n",
    "    ekf = KalmanFilterExtended(np.array([thymio_pos_x * -1, thymio_pos_y, ((thymio_theta + 180) % 360) * np.pi / 180]), u)\n",
    "```\n",
    "\n",
    "The function `run_EKF` ensures that the extened filter is intiated the filter steps are correctly called in the correct order. The code snippet below is part of it.\n",
    "\n",
    "```python\n",
    "    ekf.prediction(dt)\n",
    "    measured_state = ekf.get_state()\n",
    "    ekf.update([pos_x, pos_y, theta], u, cam)\n",
    "    measurement_update = ekf.get_state()\n",
    "``` \n",
    "\n",
    "It helps us to have the filter run in `main.py` in a loop throughout the entire mission of the robot as\n",
    "\n",
    "```python\n",
    "    measured_state, kidnap = run_EKF(ekf, thymio_pos_x * -1, thymio_pos_y, theta, u, cam=camera)\n",
    "```\n",
    "\n",
    "Additionally, we have set parameters that define *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. This mode is apart from the *kidnapping* condition from the Computer Vision side, and we decided to have two modes of kidnap detection to account for a variety of possible ways to kidnap the robot. Specifically, we have set the threshold for orientation and position to be 100 mm and 1 rad, and exceeding either (or both) of these values would be considered getting kidnaped.\n",
    "\n",
    "```python\n",
    "    if dpos > 100 or dtheta > 1:\n",
    "        kidnap = True\n",
    "        print(\"Thymio is being kidnapped (moved too far or rotated too much)\")\n",
    "        cur_t = time.time()\n",
    "        ekf.count_time(cur_t)\n",
    "        ekf.get_state()\n",
    "    else:\n",
    "        kidnap = False\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also added a `draw_confidence_ellipse()` function that draws confidence ellipse taking from the first two diagonal components of the $P_{k}$ matrix as a means to visualize the uncertainty that the system has regarding the robot's pose.\n",
    "```python\n",
    "    P = get_cov().[:2, :2]\n",
    "```\n",
    "\n",
    "The method of implementing it was referenced from Github tutorial on [Robot Motion](https://github.com/jotaraul/jupyter-notebooks-for-robotics-courses/blob/master/3-Robot%20motion.ipynb) and [The Kalman Filter](https://engineeringmedia.com/controlblog/the-kalman-filter)\n",
    "\n",
    "Then the extracted $P_{k}$ matrix will be\n",
    "\n",
    "$P_{k} = \\begin{bmatrix}\n",
    "\\sigma_{xx}^2 & \\sigma_{xy}^2 \\\\ \n",
    "\\sigma_{yx}^2 & \\sigma_{yy}^2 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We computed the eigenvalues and eigenvectors of it as the major and minor axis of the confidence ellipse is determined by the eigenvalues and the angles of the confidence ellipse is determined by the eigenvectors.\n",
    "\n",
    "```python\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(P)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "``` \n",
    "\n",
    "Then, we used ChatGPT to add chi-squared confidence interval to the ellipse computing function to account for the nonlinearity of the system.\n",
    "\n",
    "```python\n",
    "    chi_squared_val = np.sqrt(5.991) \n",
    "    amplification = 1\n",
    "    major_axis = chi_squared_val * np.sqrt(eigenvalues[0]) * amplification\n",
    "    minor_axis = chi_squared_val * np.sqrt(eigenvalues[1]) * amplification\n",
    "\n",
    "    center = (int(mean[0]), int(mean[1]))\n",
    "    axes = (int(major_axis), int(minor_axis))\n",
    "    cv2.ellipse(frame, center, axes, angle, 0, 360, color, 2)\n",
    "``` \n",
    "In `main.py`, it is called as\n",
    "```python\n",
    "    draw_confidence_ellipse(frame_aruco_and_corners, ekf.get_cov()[:2, :2], [x, y], color=(0, 255, 255))\n",
    "```\n",
    "in the loop of updating every measurement so that the ellipse is also plotted dynamically.\n",
    "This visualization of positional uncertainty in 2D space provides better intuition of how the uncertainty changes during the mission and different modes such as kidnapping and switching between vision and no vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_5_'></a>[IV.5 Simulation](#toc5_5_)\n",
    "\n",
    "The simulation on filtering was done several times on the path planning scenario found in the previous section of Global Navigation. One of the examples is shown below, where the `run_EKF` receives a set $x$ and $y$ coordinate of the planned path in each iteration and applies filtering from the `KalmanFilterExtended` class functions. The code snippet of this can be found in `KFsim.py`. The purpose of this simulation is to observe whether the filtering is successfully running throughout the entire travel sequence and to observe different results in two different conditions: when Thymio has vision and when it does not. The simulation is done by imposing random noises of normal distribution on the wheel sensors and camera vision and compare the path the robot would take with the true path the path planning provides. When the simulation confirms that the filtering is operating well, we could integrate the algorithm into our `main.py`.\n",
    "\n",
    "<img src=\"report-images/EKF_simulation.png\" width=\"571\"/>\n",
    "<img src=\"report-images/kalman_simu.png\" width=\"571\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[IV. Main Alogrithm](#toc4_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now explain how the main algorithm works (whole code can be found in [`main.py`](/main.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the algorithm the # of obstacles has to be provided with the command-line argument (this will be used to extract global obstacles):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_OBSTACLES = int(sys.argv[1])\n",
    "\n",
    "if NUMBER_OF_OBSTACLES < 0:\n",
    "    print(\"Please provide a number of obstacles greater than 0\")\n",
    "    os.close(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Computer Vision instance is initialized and started. The `while` loop was added in order to avoid noise from the camera when it starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = ComputerVision(camera_index=1, robot_id=4, goal_id=8)\n",
    "corners = []\n",
    "\n",
    "ret, frame = vision.cam.read()\n",
    "# While loop to avoid noisy detections when camera starts\n",
    "while True:\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame. Exiting.\")\n",
    "\n",
    "    frame_masked = vision.apply_color_mask(frame,  mask_color='r')  # We apply a red mask to detect only red obstacles\n",
    "    frame_with_vectors, markers_data, marker_ids, conversion_factor = get_frame_with_vectors(vision, frame)\n",
    "    edges = vision.detect_edges(frame_masked)  # We detect the edges on the masked frame using Canny\n",
    "    corners = vision.get_corners_and_shape_edges(edges)  # We obtain shapes and corners by approxPolyDP\n",
    "\n",
    "    ret, frame = vision.cam.read()\n",
    "\n",
    "    # If all the obstacles are detected, we can break the loop\n",
    "    if len(corners) == NUMBER_OF_OBSTACLES:\n",
    "        break\n",
    "\n",
    "# Conversion from pixels to mm\n",
    "corners_mm = [np.array(shape) / conversion_factor for shape in corners]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the mask is applied, the edges and corners are then extracted from the image. The conversion factor is used to convert the pixel values to mm. The loop is broken when all the obstacles are detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, Thymio's position and goal position are extracted from the markers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thymio_pos_x, thymio_pos_y, thymio_theta = get_thymio_localisation(markers_data)\n",
    "goal_pos = get_goal_position(markers_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is done with the helper functions added in the same file for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global path is then found using the path planning algorithm explained in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_goal = deepcopy(goal_pos)\n",
    "global_obstacles = deepcopy(corners_mm)\n",
    "navigation = Navigation(global_obstacles, [thymio_pos_x, thymio_pos_y], global_goal)\n",
    "global_path = navigation.get_shortest_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thymio's position and orientation are set to the initial values got from the markers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thymio = Thymio()\n",
    "thymio.set_position(np.array([thymio_pos_x * -1, thymio_pos_y]))\n",
    "thymio.set_orientation(((thymio_theta + 180) % 360) * np.pi / 180)\n",
    "goal_pos[0] = -goal_pos[0]\n",
    "thymio.set_goal(goal_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was already explained in the **Motion Control & Local Navigation** section, the coordinate system is reversed and both the goal and Thymio's positions are flipped on the $x$-axis.\n",
    "Thymio's orientation is also flipped so it aligns with the correct $x$-axis. It is then transformed to radians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Extended Kalman Filter is then initialized with the initial position and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = thymio.get_wheels_speed()\n",
    "ekf = KalmanFilterExtended(np.array([thymio_pos_x * -1, thymio_pos_y, ((thymio_theta + 180) % 360) * np.pi / 180]), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop of the program is then started. The loop is running until the robot reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not thymio.is_on_goal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local navigation is applied as it was mentioned in the **Motion Control & Local Navigation** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not thymio.is_kidnapped and np.any(thymio.get_horizontal_sensors() > thymio.OBSTACLE_THRESHOLD):\n",
    "    thymio.local_navigation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges and corners are detected with the Computer Vision algorithm in the same way as it was done at the beginning of the program. Then the results are drawn on the frame for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_obstacles(frame_aruco_and_corners, navigation, conversion_factor)\n",
    "draw_path(frame_aruco_and_corners, drawing_path, conversion_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the check for covered camera is done to detect if we can detect the Thymio and the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_covered = vision.is_camera_covered(marker_ids)\n",
    "    if camera_covered:\n",
    "        goal_pos = get_goal_position(markers_data)\n",
    "        if goal_pos is None:\n",
    "            print(\"Camera is covered!\")\n",
    "            camera = False\n",
    "        else:\n",
    "            print(\"Thymio is being kidnapped (marker not detected)\")\n",
    "            thymio.kidnap()\n",
    "            continue\n",
    "    else:\n",
    "        camera = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the camera is covered but the goal is still visible it means that the Thymio is kidnapped and the `kidnap()` function is called. The `continue` statement is used to skip the rest of the loop and go to the next iteration until the Thymio is visible again and the new global path can be recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if thymio.is_kidnapped:\n",
    "    thymio.recover()\n",
    "    navigation = Navigation(global_obstacles, [thymio_pos_x, thymio_pos_y], global_goal)\n",
    "    global_path = navigation.get_shortest_path()\n",
    "    drawing_path = deepcopy(global_path)\n",
    "    check_num = 0\n",
    "\n",
    "    u = thymio.get_wheels_speed()\n",
    "    ekf = KalmanFilterExtended(np.array([thymio_pos_x * -1, thymio_pos_y, ((thymio_theta + 180) % 360) * np.pi / 180]), u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second figure shows the confidence ellipses, where the purple ellipses represent with vision and the yellow ellipses represent without vision. For visualization purposes, the purple ellipses were amplified 150-fold and the yellow ellipses were amplified 50-fold. Even with higher amplification factor, the confidence ellipses are significantly smaller for with vision case than for the without vision case, which means that the system tend to have much more certainty when the camera is not covered than when the camera is covered. The yellow ellipses grow with time because the longer the robot is blocked from vision, the more uncertain further steps will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Conclusion](#toc6_)\n",
    "This project was a valuable opportunity to apply the theory we learned about mobile robotics. We tried to cover and use the concepts of the course as much as we could, from computer vision, to navigation and motion control uing Kalman filtering. \n",
    "This kind of project is always a good way to reinforce our ability to work as a group and to use collaborative tools like git.\n",
    "The split of the tasks made debugging easier until the first integrated runs. \n",
    "\n",
    "We faced some challenges:\n",
    "- Polygon vertices detection\n",
    "- Completeness of path planning\n",
    "- Robutness of Kalman filter\n",
    "\n",
    "But we were able to work together and solve them without too much difficulty.\n",
    "\n",
    "At the end, although one might say our Thymio is rather slow, it never fails to reach its goal."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
