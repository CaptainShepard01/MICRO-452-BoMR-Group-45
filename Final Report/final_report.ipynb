{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[**MICRO-452 Final Report**](#toc0_)\n",
    "### <a id='toc1_1_1_'></a>[*Group 45: Anton Balykov, Teo Halevi, Cyprien Lacassagne, Seonmin Park*](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Table of Contents](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Project Objective](#toc3_)    \n",
    "- [I. Computer Vision](#toc4_)    \n",
    "  - [I.1 Introduction](#toc4_1_)    \n",
    "  - [I.2 Camera Set Up](#toc4_2_)    \n",
    "  - [I.3 Global Obstables and Thymio Detection](#toc4_3_)    \n",
    "- [II. Global Navigation](#toc5_)    \n",
    "  - [II.1 Introduction](#toc5_1_)    \n",
    "  - [II.2 Path Planning](#toc5_2_)    \n",
    "  - [II.3 Algorithm](#toc5_3_)    \n",
    "- [III. Local Navigation](#toc6_)    \n",
    "  - [III.1 Introduction](#toc6_1_)    \n",
    "  - [III.1 Local Avoidance](#toc6_2_)    \n",
    "- [IV. Filtering](#toc7_)    \n",
    "  - [IV.1 Introduction](#toc7_1_)    \n",
    "  - [IV.2 Methodology](#toc7_2_)    \n",
    "  - [IV.3 Dynamics of Thymio](#toc7_3_)    \n",
    "  - [IV.4 Design and Implementation of Filter](#toc7_4_)    \n",
    "  - [IV.5 Simulation](#toc7_5_)    \n",
    "- [Conclusion](#toc8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Project Objective](#toc0_)\n",
    "\n",
    "This project leverages the Thymio robot to explore and integrate key concepts introduced in the Introduction to Mobile Robotics course. We hereby focus on implementing and demonstrating core components: computer vision, global navigation, motion control, local navigation, and filtering. The mission of the robot is to navigate itself from the designated start position to the specified goal position, successfully avoiding fixed obstacles as well as dynamically placed one. Achieving this requires precise motion control, robust obstable detection, and efficient filtering technique. Such practical application of the five above-mentioned elements would reflect our team's comprehensive understanding of mobile robotics principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[I. Computer Vision](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[I.1 Introduction](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc4_2_'></a>[I.2 Camera Set Up](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc4_3_'></a>[I.3 Global Obstables and Thymio Detection](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[II. Global Navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[II.1 Introduction](#toc0_)\n",
    "\n",
    "One main component of this project is navigation. Here, we discuss about our implementation of the global navigation. The main objectives of the global navigation is to find a collision-free path from a source and a goal, on a map filled with polygons acting as obtacles. The robot has to reach the goal while avoiding the obstacles. The strategy relies on a metric map and visibility graph.\n",
    "\n",
    "## <a id='toc5_2_'></a>[II.2 Path Planning](#toc0_)\n",
    "\n",
    "The class `Navigation` is constructed by receiving the position of the robot (source), the goal, and the corners of each obstacle. The latter are detected using polygon approximation in the class `ComputerVision`. For each obstacle, they are grouped into a list of points, representing its respective vertices.<br>\n",
    "\n",
    "However, before going into graph creation, the polygons need to be expanded in order to account for the size of the robot. Otherwise, the robot would be considered as a single point object and bump into the obstacles.\n",
    "\n",
    "Then, a visibility graph is built using the open-source library `pyvisgraph`.\n",
    "Finally, the path is computed using the function `graph.get_shortest_path`, and returned to the main function to assess the robot's deviation from the goal.\n",
    "\n",
    "## <a id='toc5_3_'></a>[II.3 Algorithm](#toc0_)\n",
    "\n",
    "The obstacles are expanded in an intuitive manner: for each vertex, a point outside of the polygon is computed based on the direction of the two adjacent edges. The distance of the expansion is simply equals to the radius (diagonal) of the robot.<br>\n",
    "\n",
    "The graph is constructed using a basic graph algorithm. Then calculations are made to filter out the connections violating free-space movement, i.e. connections between two vertices going through one or more obstacles.<br>\n",
    "This yields a visibility graph connecting nodes that will form candidate paths from source to goal.\n",
    "Then Dijkstra's algorithm is then used to find the shortest path in the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[III. Local Navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[III.1 Introduction](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    "## <a id='toc6_2_'></a>[III.1 Local Avoidance](#toc0_)\n",
    "\n",
    "bla bla bla\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[IV. Filtering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[IV.1 Introduction](#toc0_)\n",
    "\n",
    "\n",
    "There is a need for a method that compensates the defect that there is no direct mapping between Thymio's sensors and the values which are used for control. The absolute position of the robot can be obtained directly from the camera and global path planning techniques, but the robot may have low precision and update rate, causing drifts when the velocity or acceleration is integrated. We can address this challenge by implementing Kalman filter as it provides a sound framework to use all available measurements in an optimal way. It relies on a model for the system structure and the effect of disturbances both on the state and on the measurements.\n",
    "\n",
    "The Kalman filter we implemented estimates the position and speed of the robot when it moves along the planned path. Specifically, we implemented extended Kalman filter to take into account the system involves nonlinear dynamics that would need to be linearized around the current estimate, focusing on local optimality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_2_'></a>[IV.2 Methodology](#toc0_)\n",
    "\n",
    "The Thymio robot moves at constant speed across a pattern made of black and white stripes of 50 mm, identical to one of the activities found in Exercise 8 of the course. The actual mean velocity of the two wheels and the mean reflected light read by the two ground sensors are sent in an event at a constant rate. The python code receives these values, updates the estimated velocity and position of the robot, and displays them continuously in a plot.\n",
    "\n",
    "\n",
    "To obtain velocity parameters, we began with estimating Thymio's speed by let it cross seven blocks that are 50 mm each with a sample rate of 0.1 seconds and marked peaks of the ground sensor meaasuremnet indicating that it has crossed evey block. With known and obtained information, we could obtain the conversion factor of 0.4 to conver Thymio's speed to mm/s.\n",
    "\n",
    "![Find Peaks](find_peaks.png \"Fine Peaks\")\n",
    "\n",
    "Now, to obtain speed variance, we observed the data where the robot was moving by plotting the robot's speed and computed average velocity and thus the speed variance by diving the average speed with the robot's speed in mm/s.\n",
    "\n",
    "![Measured Vel](measured_vel.png \"Measured Vel\")\n",
    "\n",
    "With the assumption that half of the variance is cause by the measurement and the other half is caused by the perturbation to the states, we obtain \n",
    "\n",
    "$q_v = r_v = 16.08621$ $mm^2/s^2$ $\\space$ where $\\space$ $\\sigma^2_{vel} = 32.17242$ $mm^2/s^2$\n",
    "\n",
    "Variances on position state and measurement as well as those for the angle for orientation ($\\sigma^2_{pos}$ and $\\sigma^2_{\\theta}$) were an arbitrary value that were tuned manually for the specific Thymio robot that is going to be used for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_3_'></a>[IV.3 Dynamics of Thymio](#toc0_)\n",
    "\n",
    "To begin with the implementation, the state space model is\n",
    "\n",
    "$\\hat{x}_{k-1|k-1} =  \\begin{bmatrix}\n",
    "                        x_{pos_{k-1}} \\\\\n",
    "                        y_{pos_{k-1}} \\\\\n",
    "                        \\theta_{k-1} \\\\\n",
    "                        v_{left_{k-1}} \\\\\n",
    "                        v_{right_{k-1}} \\\\\n",
    "                        \\end{bmatrix}$\n",
    "\n",
    "Under control input vector,\n",
    "\n",
    "$ u =[v_{trans}, v_{rot}] $, $\\space$ where $\\space$ $v_{tran} = \\frac{v_{left} + v_{right}}{2} $ $\\space$ and $\\space$ $ v_{rot} = \\frac{v_{left} - v_{right}}{l_{base}} $\n",
    "\n",
    "<img src=\"Thymio_odometry.png\" width=\"571\"/>\n",
    "\n",
    "Then, we can see that the robot's new position and orientation are predicted in the following way\n",
    "\n",
    "$x_{pos_{k}} = x_{pos_{k-1}} + v_{trans_{k}} \\cdot \\cos(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$y_{pos_{k}} = y_{pos_{k-1}} + v_{trans_{k}} \\cdot \\sin(\\theta_{k-1}) \\cdot dk$\n",
    "\n",
    "$\\theta_{k} = \\theta_{k-1} + v_{rot_{k}} \\cdot dk$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_4_'></a>[IV.4 Design and Implementation of Filter](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the steps of the extended Kalman filter we implemented, we would like to disclaim that the notations and formulas were referenced from lecture slides 8 of the Mobile Robotics course and online tutorial at https://automaticaddison.com/extended-kalman-filter-ekf-with-python-code-example/.\n",
    "\n",
    "We have introduced that the extended Kalman filter takes into account nonlinear system dynamics in the previous sections. In order to make prediction of the next state, We use a nonlinear motion model $f(x, u)$. That is, we can formulate their relation as:\n",
    "\n",
    "$\\hat{x}_{k|k-1} = f(\\hat{x}_{k-1|k-1}, u_k)$\n",
    "\n",
    "Our $f(x, u)$ is then introduced as\n",
    "\n",
    "$f(\\hat{x}_{k-1|k-1}, u_k) = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & 0 & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "In this prediction step of the filter, we make a use of the predicted covariance matrix $P_{k-1|k-1}$.\n",
    "\n",
    "$P_{k-1|k-1} = \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as each column corresponds to the state variables, from the first column to the last column, being $x_{pos_{k-1}}$, $y_{pos_{k_1}}$, $\\theta_{k-1} $, $v_{left_{k-1}}$, and $v_{right_{k-1}}$, respectively.\n",
    "\n",
    "The updated (current) covariance matrix is as follows:\n",
    "\n",
    "$P_{k|k-1} = F_{k}P_{k-1|k-1}F_{k}^{T} + Q_{k}$\n",
    "\n",
    "where $F_{k}$ is the Jacobian of $f_{k}$ matrix, and $Q_{k}$ is the noise covariance matrix of the state model. The $Q_{k}$ matrix specifically represents the degree or the extend of how much the actual motion of Thymio deviates from its assumed state space model which would be the path that it should follow for our case.\n",
    "\n",
    "We can write the two matrices as below:\n",
    "\n",
    "$F_{k|k-1} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -v_{trans} \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\cos(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 1 & v_{trans} \\cdot \\cos(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk & 0.5 \\cdot \\sin(\\theta_{k-1}) \\cdot dk \\\\ \n",
    "0 & 0 & 1 & -dk/l_{base} & dk/l_{base} \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "$Q_{k|k-1} = \\alpha \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\alpha$ is a scaling factor, which we have initialized as $\\alpha =10$, such that \n",
    "\n",
    "$Q_{k-1|k-1} = 10 \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "We set $\\alpha$ large enough so that the filter tracks large changes in the wheel sensor measurements more closely as if it was smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the updating process of the extended Kalman filter, we make a use of the term *innovation* which is the difference between the actual measurements and predicted observations. Let $z_{k}$ be the observation vector and $i_{k}$ be the innovation term. Then, we can compute:\n",
    "\n",
    "$i_{k} = z_{k} - H_{k}\\hat{x}_{k-1}$ \n",
    "\n",
    "where $H_{k}$ is the measurement matrix that is used to convert the predicted state estimate at time $k$ into predicted sensor measurements at time $k$.\n",
    "\n",
    "In this project, we have two cases that can happen. One of them is when Thymio gets information from the camera, we name it as \"when it has vision,\" and the other is when it does not get information from the camera, \"when it has no vision,\" specifically when the camera is covered.\n",
    "\n",
    "When it has vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision, \n",
    "\n",
    "$H_{k} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & 1 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "as the robot will have to rely only on its wheel sensor measurements in this scenario.\n",
    "\n",
    "With the innovation $y_{k}$, we obtain the innovation covariance matrix $S_{k}$ to be used for the computation of Kalman Gain $K_{k}$.\n",
    "\n",
    "$S_{k} = H_{k}P_{k|k-1}H_{k}^{T} + R_{k}$\n",
    "\n",
    "where $R_{k}$ is the sensor measurement noise covariance matrix. It also has different dimensionality depending on if Thymio has vision or not.\n",
    "\n",
    "When it has vision,\n",
    "\n",
    "$R_{k} = \\beta \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{pos} & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & \\sigma^2_{pos} & 0 & 0 & 0 \\\\ \n",
    "0 & 0 & \\sigma^2_{\\theta} & 0 & 0 \\\\ \n",
    "0 & 0 & 0 & \\sigma^2_{vel} & 0 \\\\ \n",
    "0 & 0 & 0 & 0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "When it does not have vision,\n",
    "\n",
    "$R_{k} = \\gamma \\cdot \\begin{bmatrix}\n",
    "\\sigma^2_{vel} & 0\\\\ \n",
    "0 & \\sigma^2_{vel} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "with the same logic of $H_{k}$ matrices, and constants $\\beta = 1$ and $\\gamma = 1$ were defined through trial and error experiments just like $\\alpha$ of the $Q_{k}$ matrix.\n",
    "\n",
    "The Kalman Gain matrix is computed as follows:\n",
    "\n",
    "$K_{k} = P_{k|k-1}H_{k}^{T}S_{k}^{-1}$\n",
    "\n",
    "and we can update our estimated state and its covariance matrix by\n",
    "\n",
    "$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_{k}i_{k}$ $\\space$ and $\\space$ $ P_{k|k} = (I - K_{k}H_{k})P_{k|k-1}$ $\\space$,  respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps are implemented as a class `KalmanFilterExtended` and the class is called in the `main.py` file where all the five components of the project are integrated.\n",
    "\n",
    "The function `run_EKF` ensures that the extened filter is intiated the filter steps are correctly called in the correct order and helps us to have it run in `main.py` in a loop throughout the entire mission of the robot. The code snippet below is part of the `run_EKF` function.\n",
    "\n",
    "Additionally, we have set parameters that defines *kidnapping* in terms of the difference in the Thymio's orientation and position exceeding the threshold values. Specifically, we have set the threshold for orientation and position to be 35cm and 60°, and exceeding either (or both) of these values would be considered getting kidnaped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Codes to be displayed here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_5_'></a>[IV.5 Simulation](#toc0_)\n",
    "\n",
    "The simulation on filtering was done several times on the plath planning scenario found in the previous section of Global Navigation. One of the examples is shown below, where the `run_EKF` receives a set $x$ and $y$ coordinate of the planned path in each iteration and applies filtering from the `KalmanFilterExtended` class functions. The code snippet of this can be found in the Appendix. The purpose of this simulation is to observe whether the filtering is successfully ran throughout the entire travel sequence and to observe different results in two different conditions: when Thymio has vision and when it does not. The simulation is done by imposing random noises of normal distribution on the wheel sensors and camera vision and compare the path the robot would take with the true path the path planning provides. When the simulation confirms that the filtering is operating well, we could integrate the algorithm into our `main.py`.\n",
    "\n",
    "<img src=\"EKF_simulation.png\" width=\"571\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Conclusion](#toc0_)\n",
    "It has been an incredible journey to embark on this project within the constrained timeline of just three weeks, replete with challenges and invaluable learning opportunities. From the very beginning, we have had to face numerous obstacles, such as technical errors and conceptual hurdles, which have pushed us to refine our problem-solving skills and deepen our understanding of mobile robotics. Despite these challenges, our team has been able to overcome setbacks and achieve results we are proud of through persistence, discipline, and a collaborative spirit.\n",
    "\n",
    "Working through the complexities of the project was as rewarding as it was demanding. From understanding the theoretical basis to the implementation of practical solutions, each step provided an opportunity to reinforce and showcase our grasp of the principles of mobile robotics. The hands-on nature of the project tested not only our knowledge but also helped us grow as engineers, fostering resilience, adaptability, and creativity in tackling real-world problems.\n",
    "\n",
    "We would like to express our deep gratitude to everyone who guided and supported us through this journey. We would like to thank Professor Mondada, who inspired us with his experience and vision, and the teaching assistants, who were always ready to help, explain doubts, and give us valuable insights. Their encouragement and mentorship played a very important role in driving the success of our project.\n",
    "\n",
    "It is not only an academic milestone but also a source of personal and professional growth, and we will be carrying the lessons learned herein as we progress in our future endeavors related to robotics and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Appendix](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_5_'></a>[Extended Kalman Filter Simulation Code](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ExtendedKF import KalmanFilterExtended\n",
    "from run_EKF import run_EKF\n",
    "\n",
    "true_path = np.array([\n",
    "    (50.00, 430.00),\n",
    "    (52.96, 398.67),\n",
    "    (55.92, 367.33),\n",
    "    (58.88, 333.02),\n",
    "    (97.36, 305.53),\n",
    "    (135.84, 278.05),\n",
    "    (173.32, 250.54),\n",
    "    (298.88, 183.69),\n",
    "    (424.44, 116.85),\n",
    "    (550.00, 50.00)\n",
    "])\n",
    "\n",
    "x_ini = np.array([50.00, 430.00, 0])\n",
    "dt = 0.1\n",
    "vl = np.array([200, 25, 50, 190, 50, 30, 100, 75, 350, 15])\n",
    "vr = vl\n",
    "pos = np.zeros((len(true_path), 3))\n",
    "\n",
    "ekf = KalmanFilterExtended(x_ini[:3], np.array([0, 0])) \n",
    "pos_withNoise = np.copy(true_path) \n",
    "vl_withNoise = vl + np.random.normal(0, 5, len(vl)) \n",
    "vr_withNoise = vr + np.random.normal(0, 5, len(vr)) \n",
    "\n",
    "pos_withNoise[1:, 0] += np.random.normal(0, 3, len(true_path)-1)\n",
    "pos_withNoise[1:, 1] += np.random.normal(0, 3, len(true_path)-1)\n",
    "\n",
    "filtered_path_cam = np.zeros_like(pos)\n",
    "filtered_path_nocam = np.zeros_like(pos)\n",
    "measured_state = ekf.get_state()\n",
    "    \n",
    "for i in range(len(true_path)):\n",
    "    z_wheel = np.array([vl_withNoise[i], vr_withNoise[i]])\n",
    "    z_vision = np.array([pos_withNoise[i, 0], pos_withNoise[i, 1], 0, z_wheel[0], z_wheel[1]])\n",
    "\n",
    "    measured_state = run_EKF(ekf, pos_withNoise[i, 0], pos_withNoise[i, 1], 0, z_wheel, dt=dt, cam=True)\n",
    "\n",
    "    filtered_path_cam[i] = measured_state[:3]\n",
    "    measured_state = run_EKF(ekf, pos_withNoise[i, 0], pos_withNoise[i, 1], 0, z_wheel, dt=dt, cam=False)\n",
    "    filtered_path_nocam[i] = measured_state[:3]\n",
    "\n",
    "obstacles = [\n",
    "    [[147, 358], [83, 358], [81, 415], [146, 414]], \n",
    "    [[547, 291], [428, 329], [549, 360]],  \n",
    "    [[402, 184], [213, 260], [299, 358]],  \n",
    "    [[77, 154], [78, 208], [141, 208], [140, 152]],  \n",
    "    [[297, 60], [300, 112], [359, 111], [356, 60]]  \n",
    "]\n",
    "\n",
    "a1_width = 841 \n",
    "a1_height = 594  \n",
    "max_x = max(true_path[:, 0])\n",
    "max_y = max(true_path[:, 1])\n",
    "\n",
    "scale_x = a1_width / max_x\n",
    "scale_y = a1_height / max_y\n",
    "\n",
    "scaled_true_path = true_path * np.array([scale_x, scale_y])\n",
    "\n",
    "scaled_pos_withNoise = pos_withNoise * np.array([scale_x, scale_y])\n",
    "scaled_filtered_path_cam = filtered_path_cam[:, :2] * np.array([scale_x, scale_y])\n",
    "scaled_filtered_path_nocam = filtered_path_nocam[:, :2] * np.array([scale_x, scale_y])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for obstacle in obstacles:\n",
    "    obstacle = np.array(obstacle) * np.array([scale_x, scale_y])\n",
    "    obstacle = np.vstack([obstacle, obstacle[0]]) \n",
    "    plt.plot(obstacle[:, 0], obstacle[:, 1], color=\"brown\", linestyle=\"-\", linewidth=2.2)\n",
    "plt.plot(scaled_true_path[0, 0], scaled_true_path[0, 1], marker='P', markersize=15, color=\"brown\", label=\"Start\")\n",
    "plt.plot(scaled_true_path[-1, 0], scaled_true_path[-1, 1], marker='X', markersize=15, color=\"red\", label=\"Goal\")\n",
    "plt.plot(scaled_true_path[:, 0], scaled_true_path[:, 1], linestyle=\"-\", marker='o', markersize=8, color=\"black\", label=\"True path\")\n",
    "plt.plot(scaled_pos_withNoise[:, 0], scaled_pos_withNoise[:, 1], linestyle=\"--\", marker='*', markersize=15, color=\"darkgreen\", label=\"Position from camera\")\n",
    "plt.plot(scaled_filtered_path_cam[:, 0], scaled_filtered_path_cam[:, 1], linestyle=\"--\", marker='s', markersize=6, color=\"deepskyblue\", label=\"Filtered position (with vision)\")\n",
    "plt.plot(scaled_filtered_path_nocam[:, 0], scaled_filtered_path_nocam[:, 1], linestyle=\"--\", marker='^', markersize=6, color=\"purple\", label=\"Filtered position (without vision)\")\n",
    "plt.xlabel(\"X Position (mm)\")\n",
    "plt.ylabel(\"Y Position (mm)\")\n",
    "plt.title(\"Extended Kalman Filter Simulation\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlim(0, a1_width+ 20)\n",
    "plt.ylim(0, a1_height + 20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
